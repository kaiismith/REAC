{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.589666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.492644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label\n",
       "count  329.000000\n",
       "mean     0.589666\n",
       "std      0.492644\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_new.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cheat:  135\n",
      "Cheat:  194\n",
      "(222, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-cheat: \", len(df[df['Label']==0]))\n",
    "print(\"Cheat: \", len(df[df['Label']==1]))\n",
    "\n",
    "# diff = abs(len(df[df['Label']==0]) - len(df[df['Label']==1]))\n",
    "\n",
    "grouped = df.groupby(df.Label)\n",
    "df1 = grouped.get_group(1)\n",
    "df2 = grouped.get_group(0)\n",
    "\n",
    "df2_1 = df2.iloc[:135,:]\n",
    "df2_2 = df2.iloc[135:194,:]\n",
    "\n",
    "df1_train = df1.iloc[:111,:]\n",
    "df2_train = df2_1.iloc[:111,:]\n",
    "\n",
    "df1_test = df1.iloc[111:135,:]\n",
    "df2_test = df2_1.iloc[111:135,:]\n",
    "\n",
    "train = pd.concat([df1_train, df2_train])\n",
    "test  = pd.concat([df1_test, df2_test, df2_2])\n",
    "\n",
    "# shuffle data\n",
    "train = train.sample(frac=1)\n",
    "test  = test.sample(frac=1)\n",
    "\n",
    "print((len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cheat in trainsets:  111\n",
      "Cheat in trainsets:  111\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-cheat in trainsets: \", len(train[train['Label']==0]))\n",
    "print(\"Cheat in trainsets: \", len(train[train['Label']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse data from string to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(series):\n",
    "    final_series = []\n",
    "    for ser in series:\n",
    "        temp = []\n",
    "        for x in \"[],\":\n",
    "            ser = ser.replace(x, \"\")\n",
    "        new_ser  = np.fromstring(ser, dtype=float, sep=\" \")\n",
    "        for i in range(0, len(new_ser), 3):\n",
    "            chunk = [new_ser[i], new_ser[i + 1], new_ser[i + 2]]\n",
    "            temp.append(chunk)\n",
    "        final_series.append(temp)\n",
    "    return np.array(final_series, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the longest sequence of vector in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for lgaze in parseData(df['Left Gaze']):\n",
    "    if len(lgaze) > max_length:\n",
    "        max_length = len(lgaze)\n",
    "        \n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad the dataset to the longest sequence size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def padData(series, length):\n",
    "    for idx, ser in enumerate(series):\n",
    "        times = math.floor(length / len(ser))\n",
    "        add = length % len(ser)\n",
    "        \n",
    "        temp = ser[::-1]\n",
    "        for _ in range(1, times):\n",
    "            series[idx] = np.append(series[idx], temp, axis=0)\n",
    "            temp = temp[::-1]\n",
    "        if add != 0:\n",
    "            series[idx] = np.append(series[idx], temp[0:add], axis=0)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D -> AveragePooling1D -> Conv1D -> AveragePooling1D -> LSTM -> Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, AveragePooling1D, LSTM, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def init_model_38():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_33():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_35():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_36():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_37():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_45():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_gaze_train = train[\"Right Gaze\"]\n",
    "left_gaze_train  = train[\"Left Gaze\"]\n",
    "right_head_train = train[\"Right HeadPose\"]\n",
    "left_head_train  = train[\"Left HeadPose\"]\n",
    "\n",
    "label_train = train['Label']\n",
    "\n",
    "right_gaze_train, left_gaze_train, right_head_train, left_head_train = parseData(right_gaze_train), parseData(left_gaze_train), parseData(right_head_train), parseData(left_head_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_head_train  = padData(left_head_train,  max_length)\n",
    "right_head_train = padData(right_head_train, max_length)\n",
    "left_gaze_train  = padData(left_gaze_train,  max_length)\n",
    "right_gaze_train = padData(right_gaze_train, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.array(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(label_train)):\n",
    "    y_train.append(\n",
    "        tf.convert_to_tensor(\n",
    "            np.reshape(tf.keras.utils.to_categorical(label_train[i], num_classes=2), (1, 2))                 \n",
    "                            )\n",
    "             )\n",
    "    \n",
    "y_train = tf.convert_to_tensor(np.vstack(y_train), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[  -1.382   28.187  -28.221 ...    6.707    2.788    7.264]\n",
      "  [   5.656   31.011   31.523 ...   10.207    3.289   10.724]\n",
      "  [   9.22    30.034   31.417 ...    7.057   -5.121    8.719]\n",
      "  ...\n",
      "  [   2.36     9.203    9.501 ...    5.435  -11.421   12.648]\n",
      "  [   9.318   14.225   17.005 ...    4.445  -11.692   12.509]\n",
      "  [  11.962   10.637   16.008 ...    6.884  -11.436   13.348]]\n",
      "\n",
      " [[  36.342   41.477   55.146 ...   11.85    42.871   44.478]\n",
      "  [  33.959   33.719   47.856 ...    7.603   32.601   33.476]\n",
      "  [  31.483   39.717   50.681 ...    5.353   36.244   36.637]\n",
      "  ...\n",
      "  [  24.252   38.53    45.527 ...   -6.84    36.337  -36.976]\n",
      "  [  20.4     31.143   37.229 ...  -11.937   32.249  -34.388]\n",
      "  [  16.02    36.52    39.879 ...  -23.793   47.498  -53.124]]\n",
      "\n",
      " [[ -85.222   70.045 -110.314 ...  -80.06    51.533  -95.212]\n",
      "  [ -89.178   70.701 -113.804 ...  -83.866   48.551  -96.906]\n",
      "  [ -84.536   66.731 -107.7   ...  -80.818   44.968  -92.487]\n",
      "  ...\n",
      "  [  93.216   38.052  100.683 ...   88.186   21.666   90.808]\n",
      "  [  77.406   64.635  100.844 ...   77.55    21.893   80.581]\n",
      "  [  95.274   36.747  102.115 ...   90.644   20.287   92.886]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ -11.622   35.133  -37.005 ...  -35.023   38.41   -51.98 ]\n",
      "  [ -26.668   63.857  -69.202 ...  -27.149   49.643  -56.582]\n",
      "  [ -26.713   67.274  -72.384 ...  -25.85    49.236  -55.61 ]\n",
      "  ...\n",
      "  [  51.817    7.835   52.406 ...   49.739  -27.464   56.817]\n",
      "  [  49.347    9.533   50.26  ...   47.575  -26.54    54.477]\n",
      "  [  43.167    2.393   43.234 ...   45.279  -32.576   55.78 ]]\n",
      "\n",
      " [[  -2.875   42.585  -42.682 ...    8.956   17.984   20.091]\n",
      "  [ -10.263   52.26   -53.258 ...   -2.129   20.443  -20.554]\n",
      "  [ -18.589   45.618  -49.26  ...  -12.624   18.877  -22.709]\n",
      "  ...\n",
      "  [ -26.549    3.457  -26.773 ...  -16.474   -9.271  -18.903]\n",
      "  [ -28.502    6.597  -29.255 ...  -18.373   -7.14   -19.711]\n",
      "  [ -26.273    2.985  -26.442 ...  -16.139   -7.485  -17.79 ]]\n",
      "\n",
      " [[ -34.284   53.754  -63.756 ...  -18.865   37.385  -41.875]\n",
      "  [ -22.722   48.951  -53.967 ...   -4.284   35.59   -35.847]\n",
      "  [ -27.689   48.103  -55.503 ...  -10.277   35.769  -37.216]\n",
      "  ...\n",
      "  [ -11.319   63.333  -64.337 ...   -0.711   37.313  -37.32 ]\n",
      "  [ -14.348   57.986  -59.735 ...    0.465   35.05    35.053]\n",
      "  [ -19.575   48.858  -52.633 ...   -1.163   33.978  -33.998]]], shape=(222, 1294, 12), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]], shape=(222, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "\n",
    "for i in range(len(left_gaze_train)):\n",
    "    x_train.append(tf.convert_to_tensor([\n",
    "                                            np.hstack(\n",
    "                                                        (left_gaze_train[i], right_gaze_train[i], left_head_train[i], right_head_train[i])\n",
    "                                                     )\n",
    "                                            ], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "x_train = tf.convert_to_tensor(np.vstack(x_train), dtype=tf.float32)\n",
    "\n",
    "print(x_train)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_46():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, padding=\"same\", activation='relu', input_shape=(max_length, 12)))\n",
    "    \n",
    "    model.add(LSTM(12))\n",
    "    \n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = 'softmax')) # We have only 2 classes: Non-cheat & Cheat\n",
    "    \n",
    "    adam = optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\Tensorflow_RTX_Ampere\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 9s 726ms/step - loss: 0.7882 - accuracy: 0.4975 - val_loss: 0.6519 - val_accuracy: 0.6087\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.7557 - accuracy: 0.5075 - val_loss: 0.6452 - val_accuracy: 0.6087\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7453 - accuracy: 0.5176 - val_loss: 0.6360 - val_accuracy: 0.6087\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7317 - accuracy: 0.5226 - val_loss: 0.6354 - val_accuracy: 0.6087\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7255 - accuracy: 0.5226 - val_loss: 0.6038 - val_accuracy: 0.6522\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.7121 - accuracy: 0.5126 - val_loss: 0.5830 - val_accuracy: 0.6522\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6992 - accuracy: 0.5176 - val_loss: 0.5781 - val_accuracy: 0.6522\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6880 - accuracy: 0.5276 - val_loss: 0.5734 - val_accuracy: 0.6957\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6769 - accuracy: 0.5477 - val_loss: 0.5675 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6680 - accuracy: 0.5578 - val_loss: 0.5670 - val_accuracy: 0.6957\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6602 - accuracy: 0.5628 - val_loss: 0.5657 - val_accuracy: 0.6957\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6508 - accuracy: 0.5729 - val_loss: 0.5626 - val_accuracy: 0.6957\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6469 - accuracy: 0.5879 - val_loss: 0.5581 - val_accuracy: 0.7826\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6377 - accuracy: 0.6080 - val_loss: 0.5511 - val_accuracy: 0.7826\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6332 - accuracy: 0.6080 - val_loss: 0.5435 - val_accuracy: 0.7391\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6285 - accuracy: 0.6131 - val_loss: 0.5142 - val_accuracy: 0.7391\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.6220 - accuracy: 0.6382 - val_loss: 0.4949 - val_accuracy: 0.7826\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6129 - accuracy: 0.6332 - val_loss: 0.4932 - val_accuracy: 0.7826\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.6086 - accuracy: 0.6332 - val_loss: 0.4925 - val_accuracy: 0.8261\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6049 - accuracy: 0.6533 - val_loss: 0.4924 - val_accuracy: 0.8261\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6006 - accuracy: 0.6633 - val_loss: 0.4916 - val_accuracy: 0.8261\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5990 - accuracy: 0.6734 - val_loss: 0.4918 - val_accuracy: 0.8261\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5955 - accuracy: 0.6834 - val_loss: 0.4941 - val_accuracy: 0.8261\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5919 - accuracy: 0.6834 - val_loss: 0.5033 - val_accuracy: 0.8261\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5877 - accuracy: 0.6935 - val_loss: 0.5136 - val_accuracy: 0.8261\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5871 - accuracy: 0.6985 - val_loss: 0.5154 - val_accuracy: 0.8261\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5907 - accuracy: 0.6834 - val_loss: 0.5178 - val_accuracy: 0.8261\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5912 - accuracy: 0.6884 - val_loss: 0.5323 - val_accuracy: 0.7826\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5845 - accuracy: 0.6734 - val_loss: 0.5319 - val_accuracy: 0.7826\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5824 - accuracy: 0.6834 - val_loss: 0.5285 - val_accuracy: 0.7826\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5792 - accuracy: 0.6935 - val_loss: 0.5239 - val_accuracy: 0.7826\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5756 - accuracy: 0.7035 - val_loss: 0.5116 - val_accuracy: 0.7826\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.5697 - accuracy: 0.7136 - val_loss: 0.4987 - val_accuracy: 0.8261\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5665 - accuracy: 0.7236 - val_loss: 0.4928 - val_accuracy: 0.8261\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5645 - accuracy: 0.7236 - val_loss: 0.4891 - val_accuracy: 0.8261\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5669 - accuracy: 0.7236 - val_loss: 0.4866 - val_accuracy: 0.8261\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5653 - accuracy: 0.7236 - val_loss: 0.4839 - val_accuracy: 0.8261\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.5614 - accuracy: 0.7186 - val_loss: 0.4657 - val_accuracy: 0.8261\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5628 - accuracy: 0.7236 - val_loss: 0.4640 - val_accuracy: 0.8261\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5602 - accuracy: 0.7286 - val_loss: 0.4593 - val_accuracy: 0.8261\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5582 - accuracy: 0.7236 - val_loss: 0.4596 - val_accuracy: 0.8261\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5578 - accuracy: 0.7286 - val_loss: 0.4612 - val_accuracy: 0.8261\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.5560 - accuracy: 0.7236 - val_loss: 0.4624 - val_accuracy: 0.8261\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5535 - accuracy: 0.7286 - val_loss: 0.4632 - val_accuracy: 0.8261\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5500 - accuracy: 0.7337 - val_loss: 0.4628 - val_accuracy: 0.8261\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5476 - accuracy: 0.7337 - val_loss: 0.4626 - val_accuracy: 0.8261\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5453 - accuracy: 0.7387 - val_loss: 0.4622 - val_accuracy: 0.8261\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.5430 - accuracy: 0.7437 - val_loss: 0.4624 - val_accuracy: 0.8261\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5406 - accuracy: 0.7387 - val_loss: 0.4630 - val_accuracy: 0.8261\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5379 - accuracy: 0.7437 - val_loss: 0.4647 - val_accuracy: 0.8261\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5355 - accuracy: 0.7437 - val_loss: 0.4662 - val_accuracy: 0.8261\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5332 - accuracy: 0.7538 - val_loss: 0.4669 - val_accuracy: 0.8261\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5290 - accuracy: 0.7538 - val_loss: 0.4671 - val_accuracy: 0.8261\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5269 - accuracy: 0.7588 - val_loss: 0.4673 - val_accuracy: 0.8261\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5242 - accuracy: 0.7588 - val_loss: 0.4672 - val_accuracy: 0.8261\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5214 - accuracy: 0.7588 - val_loss: 0.4672 - val_accuracy: 0.8261\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5205 - accuracy: 0.7638 - val_loss: 0.4672 - val_accuracy: 0.8261\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.5163 - accuracy: 0.7688 - val_loss: 0.4659 - val_accuracy: 0.8261\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.5321 - accuracy: 0.7638 - val_loss: 0.4618 - val_accuracy: 0.8261\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.5327 - accuracy: 0.7387 - val_loss: 0.4730 - val_accuracy: 0.8261\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.5326 - accuracy: 0.7337 - val_loss: 0.4748 - val_accuracy: 0.8261\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5298 - accuracy: 0.7588 - val_loss: 0.4775 - val_accuracy: 0.8261\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5270 - accuracy: 0.7688 - val_loss: 0.4811 - val_accuracy: 0.8261\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5201 - accuracy: 0.7789 - val_loss: 0.4878 - val_accuracy: 0.8261\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5151 - accuracy: 0.7839 - val_loss: 0.4928 - val_accuracy: 0.8261\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5112 - accuracy: 0.7839 - val_loss: 0.4948 - val_accuracy: 0.8261\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.5068 - accuracy: 0.7889 - val_loss: 0.4959 - val_accuracy: 0.8261\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5003 - accuracy: 0.8040 - val_loss: 0.4943 - val_accuracy: 0.8261\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5006 - accuracy: 0.8040 - val_loss: 0.4880 - val_accuracy: 0.8261\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4983 - accuracy: 0.8090 - val_loss: 0.4769 - val_accuracy: 0.8261\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4963 - accuracy: 0.8141 - val_loss: 0.4691 - val_accuracy: 0.8261\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4927 - accuracy: 0.8191 - val_loss: 0.4672 - val_accuracy: 0.8261\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4908 - accuracy: 0.8191 - val_loss: 0.4658 - val_accuracy: 0.8261\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.4886 - accuracy: 0.8291 - val_loss: 0.4652 - val_accuracy: 0.8261\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4864 - accuracy: 0.8291 - val_loss: 0.4644 - val_accuracy: 0.8261\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.4842 - accuracy: 0.8291 - val_loss: 0.4630 - val_accuracy: 0.8261\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4826 - accuracy: 0.8241 - val_loss: 0.4615 - val_accuracy: 0.8261\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4811 - accuracy: 0.8241 - val_loss: 0.4603 - val_accuracy: 0.8261\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4796 - accuracy: 0.8241 - val_loss: 0.4596 - val_accuracy: 0.8261\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4778 - accuracy: 0.8291 - val_loss: 0.4596 - val_accuracy: 0.8261\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.4764 - accuracy: 0.8291 - val_loss: 0.4594 - val_accuracy: 0.8261\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4743 - accuracy: 0.8291 - val_loss: 0.4594 - val_accuracy: 0.8261\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4723 - accuracy: 0.8241 - val_loss: 0.4593 - val_accuracy: 0.8261\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4703 - accuracy: 0.8241 - val_loss: 0.4598 - val_accuracy: 0.8261\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4670 - accuracy: 0.8291 - val_loss: 0.4638 - val_accuracy: 0.8261\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4651 - accuracy: 0.8291 - val_loss: 0.4724 - val_accuracy: 0.8261\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4633 - accuracy: 0.8291 - val_loss: 0.4756 - val_accuracy: 0.8261\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4607 - accuracy: 0.8392 - val_loss: 0.4754 - val_accuracy: 0.8261\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4590 - accuracy: 0.8342 - val_loss: 0.4728 - val_accuracy: 0.8261\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.4571 - accuracy: 0.8392 - val_loss: 0.4689 - val_accuracy: 0.8261\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4549 - accuracy: 0.8392 - val_loss: 0.4639 - val_accuracy: 0.8261\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4523 - accuracy: 0.8392 - val_loss: 0.4587 - val_accuracy: 0.8261\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4503 - accuracy: 0.8392 - val_loss: 0.4567 - val_accuracy: 0.8261\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4480 - accuracy: 0.8392 - val_loss: 0.4575 - val_accuracy: 0.8261\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4460 - accuracy: 0.8392 - val_loss: 0.4580 - val_accuracy: 0.8261\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4459 - accuracy: 0.8492 - val_loss: 0.4630 - val_accuracy: 0.8261\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.4435 - accuracy: 0.8492 - val_loss: 0.4679 - val_accuracy: 0.8261\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.4420 - accuracy: 0.8442 - val_loss: 0.4688 - val_accuracy: 0.8261\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4402 - accuracy: 0.8392 - val_loss: 0.4684 - val_accuracy: 0.8261\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4390 - accuracy: 0.8392 - val_loss: 0.4670 - val_accuracy: 0.8261\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 1s/step - loss: 1.0052 - accuracy: 0.4573 - val_loss: 0.8686 - val_accuracy: 0.5217\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.8991 - accuracy: 0.4874 - val_loss: 0.7281 - val_accuracy: 0.5652\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.8394 - accuracy: 0.4975 - val_loss: 0.7296 - val_accuracy: 0.5652\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7872 - accuracy: 0.5327 - val_loss: 0.7127 - val_accuracy: 0.5652\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7627 - accuracy: 0.5075 - val_loss: 0.7061 - val_accuracy: 0.5217\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.7504 - accuracy: 0.5126 - val_loss: 0.6905 - val_accuracy: 0.5652\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.7304 - accuracy: 0.5276 - val_loss: 0.6850 - val_accuracy: 0.5217\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.7096 - accuracy: 0.5477 - val_loss: 0.6923 - val_accuracy: 0.5217\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6937 - accuracy: 0.5477 - val_loss: 0.6983 - val_accuracy: 0.4783\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6845 - accuracy: 0.5477 - val_loss: 0.6820 - val_accuracy: 0.5217\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6768 - accuracy: 0.5528 - val_loss: 0.6799 - val_accuracy: 0.5217\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6706 - accuracy: 0.5628 - val_loss: 0.6846 - val_accuracy: 0.5652\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6641 - accuracy: 0.5779 - val_loss: 0.6870 - val_accuracy: 0.5217\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6590 - accuracy: 0.5879 - val_loss: 0.6888 - val_accuracy: 0.5217\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6525 - accuracy: 0.5980 - val_loss: 0.6918 - val_accuracy: 0.5217\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6471 - accuracy: 0.6080 - val_loss: 0.6958 - val_accuracy: 0.5217\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6410 - accuracy: 0.5930 - val_loss: 0.6960 - val_accuracy: 0.5217\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6357 - accuracy: 0.6382 - val_loss: 0.6970 - val_accuracy: 0.4348\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6292 - accuracy: 0.6432 - val_loss: 0.6956 - val_accuracy: 0.4348\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6234 - accuracy: 0.6633 - val_loss: 0.6934 - val_accuracy: 0.4348\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6193 - accuracy: 0.6633 - val_loss: 0.6927 - val_accuracy: 0.4348\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6156 - accuracy: 0.6633 - val_loss: 0.6956 - val_accuracy: 0.4348\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6119 - accuracy: 0.6633 - val_loss: 0.7012 - val_accuracy: 0.4348\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6077 - accuracy: 0.6683 - val_loss: 0.7108 - val_accuracy: 0.5652\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6024 - accuracy: 0.6834 - val_loss: 0.7316 - val_accuracy: 0.5652\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5995 - accuracy: 0.6784 - val_loss: 0.7416 - val_accuracy: 0.6087\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5920 - accuracy: 0.7035 - val_loss: 0.7494 - val_accuracy: 0.5652\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5851 - accuracy: 0.7136 - val_loss: 0.7603 - val_accuracy: 0.5217\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5885 - accuracy: 0.6985 - val_loss: 0.7442 - val_accuracy: 0.5652\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5786 - accuracy: 0.6985 - val_loss: 0.7447 - val_accuracy: 0.5652\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5743 - accuracy: 0.7186 - val_loss: 0.7461 - val_accuracy: 0.5217\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5727 - accuracy: 0.7286 - val_loss: 0.7512 - val_accuracy: 0.5217\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5660 - accuracy: 0.7337 - val_loss: 0.7538 - val_accuracy: 0.5217\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5617 - accuracy: 0.7286 - val_loss: 0.7432 - val_accuracy: 0.5217\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5546 - accuracy: 0.7337 - val_loss: 0.7351 - val_accuracy: 0.5217\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5546 - accuracy: 0.7387 - val_loss: 0.7348 - val_accuracy: 0.5217\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5503 - accuracy: 0.7487 - val_loss: 0.7393 - val_accuracy: 0.5217\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5453 - accuracy: 0.7487 - val_loss: 0.7462 - val_accuracy: 0.4783\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5411 - accuracy: 0.7487 - val_loss: 0.7456 - val_accuracy: 0.4783\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5360 - accuracy: 0.7638 - val_loss: 0.7430 - val_accuracy: 0.4783\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5317 - accuracy: 0.7638 - val_loss: 0.7431 - val_accuracy: 0.4783\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5290 - accuracy: 0.7739 - val_loss: 0.7402 - val_accuracy: 0.4783\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5266 - accuracy: 0.7789 - val_loss: 0.7350 - val_accuracy: 0.5652\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5179 - accuracy: 0.7990 - val_loss: 0.7283 - val_accuracy: 0.6087\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5167 - accuracy: 0.7839 - val_loss: 0.7242 - val_accuracy: 0.6087\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5158 - accuracy: 0.7889 - val_loss: 0.7173 - val_accuracy: 0.6087\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5080 - accuracy: 0.7839 - val_loss: 0.7088 - val_accuracy: 0.6087\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5069 - accuracy: 0.7889 - val_loss: 0.7030 - val_accuracy: 0.5652\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5030 - accuracy: 0.8090 - val_loss: 0.6958 - val_accuracy: 0.6087\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5010 - accuracy: 0.7940 - val_loss: 0.6943 - val_accuracy: 0.6087\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5000 - accuracy: 0.8040 - val_loss: 0.6942 - val_accuracy: 0.6087\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4964 - accuracy: 0.8040 - val_loss: 0.6936 - val_accuracy: 0.6087\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4942 - accuracy: 0.7789 - val_loss: 0.6903 - val_accuracy: 0.6087\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4916 - accuracy: 0.7789 - val_loss: 0.6929 - val_accuracy: 0.6087\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4861 - accuracy: 0.8141 - val_loss: 0.6777 - val_accuracy: 0.6522\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4845 - accuracy: 0.7990 - val_loss: 0.6757 - val_accuracy: 0.6522\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4802 - accuracy: 0.8141 - val_loss: 0.6750 - val_accuracy: 0.6522\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4826 - accuracy: 0.8141 - val_loss: 0.6711 - val_accuracy: 0.6522\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4760 - accuracy: 0.8241 - val_loss: 0.6634 - val_accuracy: 0.6522\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4788 - accuracy: 0.7990 - val_loss: 0.6722 - val_accuracy: 0.6087\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4748 - accuracy: 0.8040 - val_loss: 0.6679 - val_accuracy: 0.6522\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4638 - accuracy: 0.8191 - val_loss: 0.6684 - val_accuracy: 0.6957\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4691 - accuracy: 0.8191 - val_loss: 0.6816 - val_accuracy: 0.6522\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4658 - accuracy: 0.8191 - val_loss: 0.6717 - val_accuracy: 0.6522\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4565 - accuracy: 0.8241 - val_loss: 0.6629 - val_accuracy: 0.6522\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4559 - accuracy: 0.8291 - val_loss: 0.6590 - val_accuracy: 0.6522\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4522 - accuracy: 0.8342 - val_loss: 0.6536 - val_accuracy: 0.6522\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4490 - accuracy: 0.8342 - val_loss: 0.6648 - val_accuracy: 0.6087\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4517 - accuracy: 0.8241 - val_loss: 0.6624 - val_accuracy: 0.6087\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4447 - accuracy: 0.8342 - val_loss: 0.6557 - val_accuracy: 0.6522\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4385 - accuracy: 0.8442 - val_loss: 0.6571 - val_accuracy: 0.6522\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4365 - accuracy: 0.8392 - val_loss: 0.6530 - val_accuracy: 0.6522\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4315 - accuracy: 0.8342 - val_loss: 0.6526 - val_accuracy: 0.6522\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4336 - accuracy: 0.8392 - val_loss: 0.6698 - val_accuracy: 0.6522\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4316 - accuracy: 0.8392 - val_loss: 0.6671 - val_accuracy: 0.6957\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4252 - accuracy: 0.8543 - val_loss: 0.6748 - val_accuracy: 0.6522\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4282 - accuracy: 0.8392 - val_loss: 0.6697 - val_accuracy: 0.6957\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4215 - accuracy: 0.8442 - val_loss: 0.6679 - val_accuracy: 0.6087\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4173 - accuracy: 0.8492 - val_loss: 0.6726 - val_accuracy: 0.6087\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4146 - accuracy: 0.8643 - val_loss: 0.6676 - val_accuracy: 0.6087\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4105 - accuracy: 0.8693 - val_loss: 0.6632 - val_accuracy: 0.6087\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4081 - accuracy: 0.8643 - val_loss: 0.6641 - val_accuracy: 0.6522\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4051 - accuracy: 0.8693 - val_loss: 0.6691 - val_accuracy: 0.6087\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3980 - accuracy: 0.8744 - val_loss: 0.6729 - val_accuracy: 0.5652\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3953 - accuracy: 0.8643 - val_loss: 0.6770 - val_accuracy: 0.5652\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3934 - accuracy: 0.8693 - val_loss: 0.6772 - val_accuracy: 0.5652\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3901 - accuracy: 0.8844 - val_loss: 0.6758 - val_accuracy: 0.6087\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3867 - accuracy: 0.8794 - val_loss: 0.6684 - val_accuracy: 0.6522\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3831 - accuracy: 0.8744 - val_loss: 0.6649 - val_accuracy: 0.6087\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3985 - accuracy: 0.8543 - val_loss: 0.6614 - val_accuracy: 0.6087\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3892 - accuracy: 0.8593 - val_loss: 0.6583 - val_accuracy: 0.6522\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3921 - accuracy: 0.8593 - val_loss: 0.6834 - val_accuracy: 0.6087\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3845 - accuracy: 0.8693 - val_loss: 0.7003 - val_accuracy: 0.5217\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3910 - accuracy: 0.8593 - val_loss: 0.6773 - val_accuracy: 0.5217\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3759 - accuracy: 0.8693 - val_loss: 0.6591 - val_accuracy: 0.5652\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4143 - accuracy: 0.8442 - val_loss: 0.6271 - val_accuracy: 0.7391\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4164 - accuracy: 0.8593 - val_loss: 0.6425 - val_accuracy: 0.7391\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4112 - accuracy: 0.8492 - val_loss: 0.6682 - val_accuracy: 0.6087\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3773 - accuracy: 0.8643 - val_loss: 0.7043 - val_accuracy: 0.5652\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4125 - accuracy: 0.8291 - val_loss: 0.7143 - val_accuracy: 0.6087\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 935ms/step - loss: 0.9626 - accuracy: 0.5025 - val_loss: 0.9332 - val_accuracy: 0.4783\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.8497 - accuracy: 0.5025 - val_loss: 0.8473 - val_accuracy: 0.4783\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.7739 - accuracy: 0.5075 - val_loss: 0.7712 - val_accuracy: 0.5217\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7258 - accuracy: 0.5176 - val_loss: 0.7173 - val_accuracy: 0.5217\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6946 - accuracy: 0.5276 - val_loss: 0.6885 - val_accuracy: 0.5217\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6785 - accuracy: 0.5628 - val_loss: 0.6731 - val_accuracy: 0.5217\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6641 - accuracy: 0.5779 - val_loss: 0.6651 - val_accuracy: 0.5652\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6534 - accuracy: 0.6080 - val_loss: 0.6631 - val_accuracy: 0.6522\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6422 - accuracy: 0.6633 - val_loss: 0.6626 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6328 - accuracy: 0.6884 - val_loss: 0.6586 - val_accuracy: 0.6522\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6257 - accuracy: 0.6633 - val_loss: 0.6501 - val_accuracy: 0.6522\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6169 - accuracy: 0.6633 - val_loss: 0.6375 - val_accuracy: 0.7391\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6160 - accuracy: 0.6633 - val_loss: 0.6336 - val_accuracy: 0.6522\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6131 - accuracy: 0.6633 - val_loss: 0.6398 - val_accuracy: 0.6522\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6053 - accuracy: 0.6834 - val_loss: 0.6364 - val_accuracy: 0.6957\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5969 - accuracy: 0.7236 - val_loss: 0.6262 - val_accuracy: 0.7391\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5887 - accuracy: 0.7186 - val_loss: 0.6426 - val_accuracy: 0.6087\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5852 - accuracy: 0.7337 - val_loss: 0.6447 - val_accuracy: 0.6087\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5789 - accuracy: 0.7538 - val_loss: 0.6260 - val_accuracy: 0.7391\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5737 - accuracy: 0.7387 - val_loss: 0.6415 - val_accuracy: 0.6087\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5658 - accuracy: 0.7839 - val_loss: 0.6441 - val_accuracy: 0.5652\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5675 - accuracy: 0.7638 - val_loss: 0.6355 - val_accuracy: 0.6957\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5592 - accuracy: 0.7588 - val_loss: 0.6302 - val_accuracy: 0.6957\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5480 - accuracy: 0.7889 - val_loss: 0.6383 - val_accuracy: 0.6087\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5553 - accuracy: 0.7789 - val_loss: 0.6411 - val_accuracy: 0.6522\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5399 - accuracy: 0.7889 - val_loss: 0.6031 - val_accuracy: 0.6522\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5610 - accuracy: 0.7387 - val_loss: 0.6040 - val_accuracy: 0.7391\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5369 - accuracy: 0.7688 - val_loss: 0.6145 - val_accuracy: 0.6522\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5427 - accuracy: 0.7889 - val_loss: 0.6119 - val_accuracy: 0.6957\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5345 - accuracy: 0.8040 - val_loss: 0.5980 - val_accuracy: 0.6522\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5248 - accuracy: 0.7739 - val_loss: 0.6185 - val_accuracy: 0.6087\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5280 - accuracy: 0.7789 - val_loss: 0.6325 - val_accuracy: 0.6522\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5167 - accuracy: 0.7940 - val_loss: 0.6363 - val_accuracy: 0.7391\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5187 - accuracy: 0.7739 - val_loss: 0.6293 - val_accuracy: 0.7391\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5085 - accuracy: 0.7990 - val_loss: 0.6147 - val_accuracy: 0.6522\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4937 - accuracy: 0.8291 - val_loss: 0.5895 - val_accuracy: 0.6522\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4915 - accuracy: 0.8342 - val_loss: 0.6006 - val_accuracy: 0.6087\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4872 - accuracy: 0.8342 - val_loss: 0.6140 - val_accuracy: 0.6087\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4741 - accuracy: 0.8392 - val_loss: 0.6268 - val_accuracy: 0.6522\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4767 - accuracy: 0.8392 - val_loss: 0.6254 - val_accuracy: 0.6957\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4724 - accuracy: 0.8342 - val_loss: 0.6190 - val_accuracy: 0.6957\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4639 - accuracy: 0.8543 - val_loss: 0.5995 - val_accuracy: 0.7391\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4567 - accuracy: 0.8543 - val_loss: 0.6025 - val_accuracy: 0.7391\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4540 - accuracy: 0.8291 - val_loss: 0.5910 - val_accuracy: 0.6957\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4451 - accuracy: 0.8492 - val_loss: 0.5946 - val_accuracy: 0.6957\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4370 - accuracy: 0.8643 - val_loss: 0.6032 - val_accuracy: 0.6957\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4260 - accuracy: 0.8794 - val_loss: 0.5799 - val_accuracy: 0.7826\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4439 - accuracy: 0.8392 - val_loss: 0.6290 - val_accuracy: 0.7391\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4646 - accuracy: 0.8141 - val_loss: 0.6046 - val_accuracy: 0.6957\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4944 - accuracy: 0.7839 - val_loss: 0.5763 - val_accuracy: 0.6957\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4845 - accuracy: 0.8090 - val_loss: 0.5477 - val_accuracy: 0.7391\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4607 - accuracy: 0.8342 - val_loss: 0.5442 - val_accuracy: 0.7826\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4675 - accuracy: 0.8141 - val_loss: 0.5421 - val_accuracy: 0.7826\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4566 - accuracy: 0.8241 - val_loss: 0.5638 - val_accuracy: 0.7391\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4287 - accuracy: 0.8392 - val_loss: 0.6040 - val_accuracy: 0.6522\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4392 - accuracy: 0.8442 - val_loss: 0.6151 - val_accuracy: 0.6957\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4220 - accuracy: 0.8392 - val_loss: 0.5622 - val_accuracy: 0.7826\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4128 - accuracy: 0.8693 - val_loss: 0.5221 - val_accuracy: 0.8261\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4193 - accuracy: 0.8442 - val_loss: 0.5456 - val_accuracy: 0.6957\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4149 - accuracy: 0.8442 - val_loss: 0.5079 - val_accuracy: 0.8261\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3889 - accuracy: 0.8543 - val_loss: 0.6004 - val_accuracy: 0.7391\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3991 - accuracy: 0.8492 - val_loss: 0.5879 - val_accuracy: 0.6957\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3852 - accuracy: 0.8593 - val_loss: 0.5909 - val_accuracy: 0.7826\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3855 - accuracy: 0.8794 - val_loss: 0.5967 - val_accuracy: 0.7826\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3750 - accuracy: 0.8794 - val_loss: 0.5457 - val_accuracy: 0.8261\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3702 - accuracy: 0.8643 - val_loss: 0.5495 - val_accuracy: 0.7826\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3634 - accuracy: 0.8844 - val_loss: 0.5370 - val_accuracy: 0.6957\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3666 - accuracy: 0.8693 - val_loss: 0.5363 - val_accuracy: 0.7391\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3625 - accuracy: 0.8744 - val_loss: 0.5121 - val_accuracy: 0.7391\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3455 - accuracy: 0.8894 - val_loss: 0.5532 - val_accuracy: 0.7391\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3605 - accuracy: 0.8844 - val_loss: 0.5212 - val_accuracy: 0.7391\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3558 - accuracy: 0.8894 - val_loss: 0.5478 - val_accuracy: 0.7826\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3368 - accuracy: 0.9045 - val_loss: 0.5363 - val_accuracy: 0.7826\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3297 - accuracy: 0.9045 - val_loss: 0.5307 - val_accuracy: 0.7826\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3339 - accuracy: 0.8945 - val_loss: 0.5879 - val_accuracy: 0.6957\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3245 - accuracy: 0.9146 - val_loss: 0.5354 - val_accuracy: 0.7826\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3412 - accuracy: 0.8995 - val_loss: 0.5598 - val_accuracy: 0.7826\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3079 - accuracy: 0.9296 - val_loss: 0.5554 - val_accuracy: 0.8261\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3230 - accuracy: 0.9146 - val_loss: 0.5511 - val_accuracy: 0.7826\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3085 - accuracy: 0.9296 - val_loss: 0.5444 - val_accuracy: 0.8261\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3262 - accuracy: 0.9095 - val_loss: 0.5578 - val_accuracy: 0.8261\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3167 - accuracy: 0.8894 - val_loss: 0.5518 - val_accuracy: 0.7826\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2904 - accuracy: 0.9246 - val_loss: 0.5418 - val_accuracy: 0.8696\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3148 - accuracy: 0.8995 - val_loss: 0.5613 - val_accuracy: 0.7826\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3015 - accuracy: 0.9045 - val_loss: 0.4828 - val_accuracy: 0.7826\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3060 - accuracy: 0.9095 - val_loss: 0.5390 - val_accuracy: 0.7826\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2982 - accuracy: 0.9196 - val_loss: 0.5071 - val_accuracy: 0.7826\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2730 - accuracy: 0.9447 - val_loss: 0.5676 - val_accuracy: 0.7826\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.2939 - accuracy: 0.9146 - val_loss: 0.5147 - val_accuracy: 0.6957\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.2725 - accuracy: 0.9347 - val_loss: 0.5969 - val_accuracy: 0.6522\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3011 - accuracy: 0.9095 - val_loss: 0.5775 - val_accuracy: 0.6957\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2832 - accuracy: 0.9246 - val_loss: 0.5876 - val_accuracy: 0.6957\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2904 - accuracy: 0.9146 - val_loss: 0.6174 - val_accuracy: 0.6522\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3065 - accuracy: 0.8995 - val_loss: 0.6446 - val_accuracy: 0.6522\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3259 - accuracy: 0.8894 - val_loss: 0.6810 - val_accuracy: 0.6957\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3319 - accuracy: 0.8794 - val_loss: 0.6635 - val_accuracy: 0.6087\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3205 - accuracy: 0.8894 - val_loss: 0.5903 - val_accuracy: 0.6522\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3154 - accuracy: 0.8894 - val_loss: 0.6432 - val_accuracy: 0.6522\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3318 - accuracy: 0.8794 - val_loss: 0.6460 - val_accuracy: 0.6087\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3269 - accuracy: 0.8844 - val_loss: 0.6566 - val_accuracy: 0.5652\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 797ms/step - loss: 0.7233 - accuracy: 0.4422 - val_loss: 0.6876 - val_accuracy: 0.5217\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.7010 - accuracy: 0.4623 - val_loss: 0.6784 - val_accuracy: 0.6087\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6885 - accuracy: 0.5276 - val_loss: 0.6786 - val_accuracy: 0.5217\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6792 - accuracy: 0.5628 - val_loss: 0.6780 - val_accuracy: 0.5217\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6714 - accuracy: 0.5879 - val_loss: 0.6763 - val_accuracy: 0.6087\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6633 - accuracy: 0.6281 - val_loss: 0.6746 - val_accuracy: 0.6087\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6544 - accuracy: 0.6281 - val_loss: 0.6690 - val_accuracy: 0.6522\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6463 - accuracy: 0.6231 - val_loss: 0.6616 - val_accuracy: 0.6957\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6374 - accuracy: 0.6231 - val_loss: 0.6550 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6293 - accuracy: 0.6533 - val_loss: 0.6486 - val_accuracy: 0.6957\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6203 - accuracy: 0.6784 - val_loss: 0.6446 - val_accuracy: 0.6957\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6105 - accuracy: 0.6985 - val_loss: 0.6413 - val_accuracy: 0.6957\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6019 - accuracy: 0.7387 - val_loss: 0.6342 - val_accuracy: 0.6957\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5888 - accuracy: 0.7286 - val_loss: 0.6261 - val_accuracy: 0.7391\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5813 - accuracy: 0.7236 - val_loss: 0.6238 - val_accuracy: 0.6957\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5719 - accuracy: 0.7387 - val_loss: 0.6260 - val_accuracy: 0.6957\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5623 - accuracy: 0.7337 - val_loss: 0.6271 - val_accuracy: 0.6957\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5532 - accuracy: 0.7538 - val_loss: 0.6285 - val_accuracy: 0.6957\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5448 - accuracy: 0.7638 - val_loss: 0.6164 - val_accuracy: 0.6522\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5301 - accuracy: 0.7889 - val_loss: 0.6055 - val_accuracy: 0.6957\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5248 - accuracy: 0.7889 - val_loss: 0.6023 - val_accuracy: 0.6522\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5136 - accuracy: 0.7688 - val_loss: 0.6096 - val_accuracy: 0.6522\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5053 - accuracy: 0.7789 - val_loss: 0.6060 - val_accuracy: 0.6522\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4942 - accuracy: 0.7990 - val_loss: 0.6040 - val_accuracy: 0.6522\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4869 - accuracy: 0.8141 - val_loss: 0.6112 - val_accuracy: 0.6087\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4828 - accuracy: 0.7839 - val_loss: 0.6118 - val_accuracy: 0.6087\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4702 - accuracy: 0.8040 - val_loss: 0.6010 - val_accuracy: 0.6522\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4597 - accuracy: 0.8040 - val_loss: 0.5984 - val_accuracy: 0.6087\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4524 - accuracy: 0.8040 - val_loss: 0.5988 - val_accuracy: 0.6087\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4434 - accuracy: 0.8241 - val_loss: 0.5981 - val_accuracy: 0.6087\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4343 - accuracy: 0.8090 - val_loss: 0.5965 - val_accuracy: 0.6957\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4303 - accuracy: 0.8241 - val_loss: 0.6053 - val_accuracy: 0.6957\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4222 - accuracy: 0.8392 - val_loss: 0.6002 - val_accuracy: 0.7391\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4110 - accuracy: 0.8291 - val_loss: 0.6005 - val_accuracy: 0.7391\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4044 - accuracy: 0.8392 - val_loss: 0.6187 - val_accuracy: 0.6957\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4281 - accuracy: 0.8241 - val_loss: 0.6095 - val_accuracy: 0.7391\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4540 - accuracy: 0.7739 - val_loss: 0.5914 - val_accuracy: 0.6087\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4298 - accuracy: 0.8141 - val_loss: 0.6181 - val_accuracy: 0.6522\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4499 - accuracy: 0.7940 - val_loss: 0.6308 - val_accuracy: 0.6087\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4269 - accuracy: 0.7889 - val_loss: 0.6698 - val_accuracy: 0.5652\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4200 - accuracy: 0.8040 - val_loss: 0.6602 - val_accuracy: 0.6522\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4038 - accuracy: 0.8241 - val_loss: 0.6411 - val_accuracy: 0.6522\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3929 - accuracy: 0.8342 - val_loss: 0.6216 - val_accuracy: 0.6522\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.84 - 0s 46ms/step - loss: 0.3902 - accuracy: 0.8643 - val_loss: 0.6049 - val_accuracy: 0.6522\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3829 - accuracy: 0.8442 - val_loss: 0.6012 - val_accuracy: 0.6957\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3711 - accuracy: 0.8492 - val_loss: 0.6046 - val_accuracy: 0.6522\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3581 - accuracy: 0.8442 - val_loss: 0.6089 - val_accuracy: 0.6522\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3491 - accuracy: 0.8442 - val_loss: 0.6277 - val_accuracy: 0.6522\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3447 - accuracy: 0.8593 - val_loss: 0.6321 - val_accuracy: 0.6522\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3369 - accuracy: 0.8593 - val_loss: 0.6267 - val_accuracy: 0.6957\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3327 - accuracy: 0.8643 - val_loss: 0.6331 - val_accuracy: 0.6957\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3265 - accuracy: 0.8693 - val_loss: 0.6362 - val_accuracy: 0.6957\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3188 - accuracy: 0.8794 - val_loss: 0.6274 - val_accuracy: 0.6957\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3135 - accuracy: 0.8643 - val_loss: 0.6237 - val_accuracy: 0.6957\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3080 - accuracy: 0.8744 - val_loss: 0.6309 - val_accuracy: 0.7391\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3037 - accuracy: 0.8794 - val_loss: 0.6213 - val_accuracy: 0.7391\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2979 - accuracy: 0.8894 - val_loss: 0.6202 - val_accuracy: 0.7391\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2907 - accuracy: 0.8844 - val_loss: 0.6334 - val_accuracy: 0.7391\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2865 - accuracy: 0.8894 - val_loss: 0.6343 - val_accuracy: 0.6957\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2789 - accuracy: 0.9045 - val_loss: 0.6081 - val_accuracy: 0.7391\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2801 - accuracy: 0.8995 - val_loss: 0.6416 - val_accuracy: 0.6522\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2749 - accuracy: 0.8894 - val_loss: 0.6951 - val_accuracy: 0.6957\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2711 - accuracy: 0.8945 - val_loss: 0.6755 - val_accuracy: 0.6957\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2577 - accuracy: 0.9045 - val_loss: 0.6817 - val_accuracy: 0.6957\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2539 - accuracy: 0.8945 - val_loss: 0.7136 - val_accuracy: 0.6957\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2473 - accuracy: 0.8945 - val_loss: 0.6953 - val_accuracy: 0.6957\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2448 - accuracy: 0.8995 - val_loss: 0.6843 - val_accuracy: 0.7391\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2347 - accuracy: 0.9196 - val_loss: 0.6842 - val_accuracy: 0.7391\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2298 - accuracy: 0.9146 - val_loss: 0.6998 - val_accuracy: 0.7391\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2247 - accuracy: 0.9246 - val_loss: 0.6881 - val_accuracy: 0.7391\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.2249 - accuracy: 0.9146 - val_loss: 0.7052 - val_accuracy: 0.6957\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2381 - accuracy: 0.9246 - val_loss: 0.6762 - val_accuracy: 0.7391\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2168 - accuracy: 0.9246 - val_loss: 0.6508 - val_accuracy: 0.7826\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2290 - accuracy: 0.9296 - val_loss: 0.7156 - val_accuracy: 0.6957\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2546 - accuracy: 0.8995 - val_loss: 0.7025 - val_accuracy: 0.7391\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2412 - accuracy: 0.9045 - val_loss: 0.7021 - val_accuracy: 0.7391\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2130 - accuracy: 0.9296 - val_loss: 0.6473 - val_accuracy: 0.7391\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2320 - accuracy: 0.9146 - val_loss: 0.6770 - val_accuracy: 0.7391\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2077 - accuracy: 0.9347 - val_loss: 0.6442 - val_accuracy: 0.7826\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2040 - accuracy: 0.9347 - val_loss: 0.6301 - val_accuracy: 0.7826\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2247 - accuracy: 0.9296 - val_loss: 0.6616 - val_accuracy: 0.7391\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1976 - accuracy: 0.9447 - val_loss: 0.6655 - val_accuracy: 0.7391\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2088 - accuracy: 0.9196 - val_loss: 0.7973 - val_accuracy: 0.6957\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1957 - accuracy: 0.9397 - val_loss: 0.7014 - val_accuracy: 0.7391\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1783 - accuracy: 0.9548 - val_loss: 0.6748 - val_accuracy: 0.7826\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1817 - accuracy: 0.9447 - val_loss: 0.6608 - val_accuracy: 0.7391\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1819 - accuracy: 0.9497 - val_loss: 0.6478 - val_accuracy: 0.7826\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1861 - accuracy: 0.9296 - val_loss: 0.6224 - val_accuracy: 0.7391\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1710 - accuracy: 0.9648 - val_loss: 0.5987 - val_accuracy: 0.7391\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1740 - accuracy: 0.9548 - val_loss: 0.5888 - val_accuracy: 0.7391\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1711 - accuracy: 0.9548 - val_loss: 0.6343 - val_accuracy: 0.7826\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1609 - accuracy: 0.9598 - val_loss: 0.6908 - val_accuracy: 0.7391\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1597 - accuracy: 0.9648 - val_loss: 0.7070 - val_accuracy: 0.7826\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1547 - accuracy: 0.9698 - val_loss: 0.6936 - val_accuracy: 0.7826\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1554 - accuracy: 0.9497 - val_loss: 0.6911 - val_accuracy: 0.7391\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1559 - accuracy: 0.9548 - val_loss: 0.6478 - val_accuracy: 0.7391\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1508 - accuracy: 0.9548 - val_loss: 0.6008 - val_accuracy: 0.7826\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1627 - accuracy: 0.9497 - val_loss: 0.8521 - val_accuracy: 0.6957\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2672 - accuracy: 0.8894 - val_loss: 0.7836 - val_accuracy: 0.7826\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2488 - accuracy: 0.9095 - val_loss: 0.8148 - val_accuracy: 0.7826\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 727ms/step - loss: 1.0629 - accuracy: 0.5176 - val_loss: 0.7006 - val_accuracy: 0.5652\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.7961 - accuracy: 0.5075 - val_loss: 0.6690 - val_accuracy: 0.6957\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.7251 - accuracy: 0.5578 - val_loss: 0.6995 - val_accuracy: 0.6087\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6867 - accuracy: 0.5879 - val_loss: 0.6618 - val_accuracy: 0.6087\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6683 - accuracy: 0.5930 - val_loss: 0.6467 - val_accuracy: 0.6087\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6539 - accuracy: 0.6181 - val_loss: 0.6261 - val_accuracy: 0.6957\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6315 - accuracy: 0.6181 - val_loss: 0.6069 - val_accuracy: 0.6957\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6233 - accuracy: 0.6080 - val_loss: 0.6158 - val_accuracy: 0.6957\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6156 - accuracy: 0.6231 - val_loss: 0.6349 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5996 - accuracy: 0.6382 - val_loss: 0.6270 - val_accuracy: 0.6957\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5903 - accuracy: 0.6633 - val_loss: 0.6230 - val_accuracy: 0.6957\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5831 - accuracy: 0.6633 - val_loss: 0.6208 - val_accuracy: 0.7391\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5869 - accuracy: 0.6633 - val_loss: 0.6257 - val_accuracy: 0.7391\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6010 - accuracy: 0.6583 - val_loss: 0.6228 - val_accuracy: 0.6957\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5797 - accuracy: 0.6935 - val_loss: 0.6502 - val_accuracy: 0.6522\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5689 - accuracy: 0.6985 - val_loss: 0.6490 - val_accuracy: 0.6957\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5624 - accuracy: 0.7085 - val_loss: 0.6328 - val_accuracy: 0.6522\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5544 - accuracy: 0.7085 - val_loss: 0.6203 - val_accuracy: 0.6957\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5444 - accuracy: 0.7337 - val_loss: 0.6026 - val_accuracy: 0.6957\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5335 - accuracy: 0.7437 - val_loss: 0.5907 - val_accuracy: 0.7826\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5272 - accuracy: 0.7437 - val_loss: 0.5921 - val_accuracy: 0.7391\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5213 - accuracy: 0.7638 - val_loss: 0.5972 - val_accuracy: 0.6957\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5144 - accuracy: 0.7588 - val_loss: 0.5948 - val_accuracy: 0.7391\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5078 - accuracy: 0.7688 - val_loss: 0.6448 - val_accuracy: 0.6522\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5093 - accuracy: 0.7889 - val_loss: 0.5894 - val_accuracy: 0.7391\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4912 - accuracy: 0.8141 - val_loss: 0.5894 - val_accuracy: 0.6957\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4825 - accuracy: 0.7889 - val_loss: 0.5833 - val_accuracy: 0.6957\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4722 - accuracy: 0.7940 - val_loss: 0.5941 - val_accuracy: 0.6957\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4620 - accuracy: 0.7940 - val_loss: 0.6031 - val_accuracy: 0.6957\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4527 - accuracy: 0.7990 - val_loss: 0.6118 - val_accuracy: 0.6522\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4386 - accuracy: 0.8040 - val_loss: 0.5913 - val_accuracy: 0.6957\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4183 - accuracy: 0.8291 - val_loss: 0.5709 - val_accuracy: 0.6957\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4079 - accuracy: 0.8543 - val_loss: 0.5688 - val_accuracy: 0.6957\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4039 - accuracy: 0.8492 - val_loss: 0.5377 - val_accuracy: 0.6522\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3894 - accuracy: 0.8543 - val_loss: 0.5446 - val_accuracy: 0.6522\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3771 - accuracy: 0.8643 - val_loss: 0.5680 - val_accuracy: 0.6522\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3695 - accuracy: 0.8593 - val_loss: 0.5811 - val_accuracy: 0.5652\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3609 - accuracy: 0.8593 - val_loss: 0.5816 - val_accuracy: 0.6087\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.3502 - accuracy: 0.8794 - val_loss: 0.5982 - val_accuracy: 0.6957\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.3475 - accuracy: 0.8794 - val_loss: 0.5951 - val_accuracy: 0.6522\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 188ms/step - loss: 0.3391 - accuracy: 0.8693 - val_loss: 0.5738 - val_accuracy: 0.6522\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 0.3525 - accuracy: 0.8593 - val_loss: 0.6203 - val_accuracy: 0.6522\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.3918 - accuracy: 0.8291 - val_loss: 0.6524 - val_accuracy: 0.6087\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4261 - accuracy: 0.8291 - val_loss: 0.6209 - val_accuracy: 0.6087\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4025 - accuracy: 0.8241 - val_loss: 0.5847 - val_accuracy: 0.6957\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3959 - accuracy: 0.8492 - val_loss: 0.6765 - val_accuracy: 0.7391\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.3964 - accuracy: 0.8291 - val_loss: 0.6129 - val_accuracy: 0.6087\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.3607 - accuracy: 0.8392 - val_loss: 0.6722 - val_accuracy: 0.6087\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3897 - accuracy: 0.8291 - val_loss: 0.6962 - val_accuracy: 0.6957\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3791 - accuracy: 0.8442 - val_loss: 0.6534 - val_accuracy: 0.6522\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3636 - accuracy: 0.8593 - val_loss: 0.6163 - val_accuracy: 0.6522\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3427 - accuracy: 0.8744 - val_loss: 0.6002 - val_accuracy: 0.6522\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3341 - accuracy: 0.8744 - val_loss: 0.5808 - val_accuracy: 0.6957\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3163 - accuracy: 0.8643 - val_loss: 0.6235 - val_accuracy: 0.6522\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3170 - accuracy: 0.8744 - val_loss: 0.5824 - val_accuracy: 0.6957\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3165 - accuracy: 0.8945 - val_loss: 0.6001 - val_accuracy: 0.6522\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3125 - accuracy: 0.9045 - val_loss: 0.5969 - val_accuracy: 0.6087\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3015 - accuracy: 0.9045 - val_loss: 0.5761 - val_accuracy: 0.7391\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2989 - accuracy: 0.9045 - val_loss: 0.6003 - val_accuracy: 0.6957\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3005 - accuracy: 0.9045 - val_loss: 0.6814 - val_accuracy: 0.6957\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3903 - accuracy: 0.8543 - val_loss: 0.6834 - val_accuracy: 0.6522\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3735 - accuracy: 0.8543 - val_loss: 0.6507 - val_accuracy: 0.6957\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3548 - accuracy: 0.8543 - val_loss: 0.6708 - val_accuracy: 0.6522\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3376 - accuracy: 0.8693 - val_loss: 0.7230 - val_accuracy: 0.6522\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3243 - accuracy: 0.8844 - val_loss: 0.6943 - val_accuracy: 0.6087\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3309 - accuracy: 0.8643 - val_loss: 0.6473 - val_accuracy: 0.6957\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3630 - accuracy: 0.8291 - val_loss: 0.6755 - val_accuracy: 0.6522\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3827 - accuracy: 0.8291 - val_loss: 0.7954 - val_accuracy: 0.6522\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3595 - accuracy: 0.8342 - val_loss: 0.8228 - val_accuracy: 0.5652\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3429 - accuracy: 0.8693 - val_loss: 0.8377 - val_accuracy: 0.6522\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3359 - accuracy: 0.8593 - val_loss: 0.8475 - val_accuracy: 0.6087\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3143 - accuracy: 0.8744 - val_loss: 0.8706 - val_accuracy: 0.5652\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3133 - accuracy: 0.8744 - val_loss: 0.8147 - val_accuracy: 0.6957\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3028 - accuracy: 0.8744 - val_loss: 0.8952 - val_accuracy: 0.6087\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2921 - accuracy: 0.8844 - val_loss: 0.9147 - val_accuracy: 0.5652\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3035 - accuracy: 0.8844 - val_loss: 0.8852 - val_accuracy: 0.6522\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2794 - accuracy: 0.9146 - val_loss: 0.8575 - val_accuracy: 0.6522\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.2651 - accuracy: 0.9246 - val_loss: 0.8652 - val_accuracy: 0.6522\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2799 - accuracy: 0.8995 - val_loss: 0.8909 - val_accuracy: 0.6522\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.2603 - accuracy: 0.9196 - val_loss: 0.8757 - val_accuracy: 0.6522\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2662 - accuracy: 0.9146 - val_loss: 0.9534 - val_accuracy: 0.5652\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2724 - accuracy: 0.9095 - val_loss: 0.9138 - val_accuracy: 0.6087\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.2809 - accuracy: 0.8995 - val_loss: 0.9003 - val_accuracy: 0.6522\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2728 - accuracy: 0.9045 - val_loss: 0.8722 - val_accuracy: 0.6087\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3080 - accuracy: 0.8794 - val_loss: 0.8470 - val_accuracy: 0.6522\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2829 - accuracy: 0.8995 - val_loss: 0.8642 - val_accuracy: 0.6522\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.2800 - accuracy: 0.8894 - val_loss: 0.7729 - val_accuracy: 0.6522\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2626 - accuracy: 0.9045 - val_loss: 0.7765 - val_accuracy: 0.5652\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.2475 - accuracy: 0.9095 - val_loss: 0.7567 - val_accuracy: 0.6087\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2382 - accuracy: 0.9296 - val_loss: 0.7704 - val_accuracy: 0.6957\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2266 - accuracy: 0.9397 - val_loss: 0.7654 - val_accuracy: 0.6522\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.2296 - accuracy: 0.9196 - val_loss: 0.8098 - val_accuracy: 0.6522\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.2060 - accuracy: 0.9296 - val_loss: 0.8050 - val_accuracy: 0.6087\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1985 - accuracy: 0.9347 - val_loss: 0.8531 - val_accuracy: 0.6087\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2100 - accuracy: 0.9246 - val_loss: 0.8944 - val_accuracy: 0.6087\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1993 - accuracy: 0.9497 - val_loss: 0.9093 - val_accuracy: 0.6087\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.1923 - accuracy: 0.9548 - val_loss: 0.9429 - val_accuracy: 0.6087\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1875 - accuracy: 0.9497 - val_loss: 0.8661 - val_accuracy: 0.6087\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1820 - accuracy: 0.9497 - val_loss: 0.8023 - val_accuracy: 0.6087\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1851 - accuracy: 0.9447 - val_loss: 0.7598 - val_accuracy: 0.6087\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 651ms/step - loss: 0.7683 - accuracy: 0.5126 - val_loss: 0.6293 - val_accuracy: 0.6522\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.7527 - accuracy: 0.5176 - val_loss: 0.6283 - val_accuracy: 0.6522\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7411 - accuracy: 0.5276 - val_loss: 0.6272 - val_accuracy: 0.6087\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.7323 - accuracy: 0.5377 - val_loss: 0.6238 - val_accuracy: 0.6087\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.7226 - accuracy: 0.5427 - val_loss: 0.6202 - val_accuracy: 0.6087\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.7143 - accuracy: 0.5578 - val_loss: 0.6165 - val_accuracy: 0.6087\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7058 - accuracy: 0.5628 - val_loss: 0.6129 - val_accuracy: 0.6087\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6984 - accuracy: 0.5477 - val_loss: 0.6092 - val_accuracy: 0.6087\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6907 - accuracy: 0.5578 - val_loss: 0.6063 - val_accuracy: 0.6087\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6841 - accuracy: 0.5628 - val_loss: 0.6036 - val_accuracy: 0.6087\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6780 - accuracy: 0.5628 - val_loss: 0.6020 - val_accuracy: 0.6087\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6738 - accuracy: 0.5628 - val_loss: 0.6014 - val_accuracy: 0.6522\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6710 - accuracy: 0.5729 - val_loss: 0.6017 - val_accuracy: 0.6522\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6655 - accuracy: 0.5628 - val_loss: 0.6032 - val_accuracy: 0.6522\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6616 - accuracy: 0.5678 - val_loss: 0.6060 - val_accuracy: 0.6522\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6585 - accuracy: 0.5930 - val_loss: 0.6098 - val_accuracy: 0.6522\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6557 - accuracy: 0.5930 - val_loss: 0.6138 - val_accuracy: 0.6522\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6532 - accuracy: 0.5980 - val_loss: 0.6162 - val_accuracy: 0.6522\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6508 - accuracy: 0.6131 - val_loss: 0.6169 - val_accuracy: 0.6522\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6483 - accuracy: 0.6181 - val_loss: 0.6168 - val_accuracy: 0.6522\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6452 - accuracy: 0.6281 - val_loss: 0.6159 - val_accuracy: 0.6522\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6423 - accuracy: 0.6332 - val_loss: 0.6151 - val_accuracy: 0.6522\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6393 - accuracy: 0.6382 - val_loss: 0.6142 - val_accuracy: 0.6522\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6376 - accuracy: 0.6533 - val_loss: 0.6130 - val_accuracy: 0.6522\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6360 - accuracy: 0.6633 - val_loss: 0.6121 - val_accuracy: 0.6522\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6338 - accuracy: 0.6633 - val_loss: 0.6115 - val_accuracy: 0.6522\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6322 - accuracy: 0.6633 - val_loss: 0.6111 - val_accuracy: 0.6522\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6300 - accuracy: 0.6633 - val_loss: 0.6107 - val_accuracy: 0.6522\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6283 - accuracy: 0.6683 - val_loss: 0.6100 - val_accuracy: 0.6522\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6265 - accuracy: 0.6784 - val_loss: 0.6092 - val_accuracy: 0.6522\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6251 - accuracy: 0.6633 - val_loss: 0.6081 - val_accuracy: 0.6522\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6232 - accuracy: 0.6784 - val_loss: 0.6071 - val_accuracy: 0.6522\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6212 - accuracy: 0.6734 - val_loss: 0.6060 - val_accuracy: 0.6522\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6189 - accuracy: 0.6784 - val_loss: 0.6049 - val_accuracy: 0.6522\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6170 - accuracy: 0.6734 - val_loss: 0.6039 - val_accuracy: 0.6522\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6154 - accuracy: 0.6533 - val_loss: 0.6029 - val_accuracy: 0.6522\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6129 - accuracy: 0.6784 - val_loss: 0.6018 - val_accuracy: 0.6522\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6110 - accuracy: 0.6734 - val_loss: 0.6012 - val_accuracy: 0.6522\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6083 - accuracy: 0.6784 - val_loss: 0.6002 - val_accuracy: 0.6522\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6058 - accuracy: 0.6734 - val_loss: 0.5994 - val_accuracy: 0.6522\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6034 - accuracy: 0.6834 - val_loss: 0.5980 - val_accuracy: 0.6522\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6013 - accuracy: 0.6884 - val_loss: 0.5958 - val_accuracy: 0.6522\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5984 - accuracy: 0.6935 - val_loss: 0.5931 - val_accuracy: 0.6522\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5955 - accuracy: 0.6985 - val_loss: 0.5905 - val_accuracy: 0.6957\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5929 - accuracy: 0.7035 - val_loss: 0.5888 - val_accuracy: 0.6957\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5900 - accuracy: 0.7085 - val_loss: 0.5889 - val_accuracy: 0.7391\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5868 - accuracy: 0.7286 - val_loss: 0.5898 - val_accuracy: 0.7391\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5841 - accuracy: 0.7437 - val_loss: 0.5918 - val_accuracy: 0.7391\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5817 - accuracy: 0.7437 - val_loss: 0.5938 - val_accuracy: 0.7391\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5792 - accuracy: 0.7487 - val_loss: 0.5946 - val_accuracy: 0.6957\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5762 - accuracy: 0.7437 - val_loss: 0.5946 - val_accuracy: 0.6957\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5741 - accuracy: 0.7387 - val_loss: 0.5938 - val_accuracy: 0.6522\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5717 - accuracy: 0.7437 - val_loss: 0.5935 - val_accuracy: 0.6522\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5690 - accuracy: 0.7437 - val_loss: 0.5941 - val_accuracy: 0.6957\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5666 - accuracy: 0.7387 - val_loss: 0.5936 - val_accuracy: 0.6957\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5655 - accuracy: 0.7588 - val_loss: 0.5926 - val_accuracy: 0.6957\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5635 - accuracy: 0.7588 - val_loss: 0.5927 - val_accuracy: 0.6957\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5612 - accuracy: 0.7588 - val_loss: 0.5912 - val_accuracy: 0.6522\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5584 - accuracy: 0.7688 - val_loss: 0.5869 - val_accuracy: 0.6522\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5566 - accuracy: 0.7739 - val_loss: 0.5857 - val_accuracy: 0.6522\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5540 - accuracy: 0.7839 - val_loss: 0.5875 - val_accuracy: 0.6522\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5512 - accuracy: 0.7889 - val_loss: 0.5879 - val_accuracy: 0.6522\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5486 - accuracy: 0.7889 - val_loss: 0.5852 - val_accuracy: 0.6522\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5446 - accuracy: 0.7839 - val_loss: 0.5820 - val_accuracy: 0.7391\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5410 - accuracy: 0.7889 - val_loss: 0.5760 - val_accuracy: 0.7391\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5385 - accuracy: 0.7889 - val_loss: 0.5692 - val_accuracy: 0.8261\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5370 - accuracy: 0.7839 - val_loss: 0.5673 - val_accuracy: 0.7826\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5351 - accuracy: 0.7839 - val_loss: 0.5728 - val_accuracy: 0.7826\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5305 - accuracy: 0.7889 - val_loss: 0.5816 - val_accuracy: 0.7826\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5293 - accuracy: 0.7990 - val_loss: 0.5771 - val_accuracy: 0.7826\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5296 - accuracy: 0.7889 - val_loss: 0.5742 - val_accuracy: 0.7826\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5215 - accuracy: 0.8191 - val_loss: 0.5690 - val_accuracy: 0.7826\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5239 - accuracy: 0.7990 - val_loss: 0.5734 - val_accuracy: 0.7826\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5155 - accuracy: 0.8090 - val_loss: 0.5768 - val_accuracy: 0.7826\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5152 - accuracy: 0.8090 - val_loss: 0.5833 - val_accuracy: 0.7391\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5091 - accuracy: 0.8291 - val_loss: 0.5871 - val_accuracy: 0.7391\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5061 - accuracy: 0.8191 - val_loss: 0.5806 - val_accuracy: 0.7391\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5033 - accuracy: 0.8191 - val_loss: 0.5685 - val_accuracy: 0.7391\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5007 - accuracy: 0.8141 - val_loss: 0.5675 - val_accuracy: 0.7391\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4977 - accuracy: 0.8342 - val_loss: 0.5710 - val_accuracy: 0.7391\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4935 - accuracy: 0.8342 - val_loss: 0.5754 - val_accuracy: 0.7826\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4919 - accuracy: 0.8090 - val_loss: 0.5738 - val_accuracy: 0.7826\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4876 - accuracy: 0.8141 - val_loss: 0.5651 - val_accuracy: 0.7826\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4838 - accuracy: 0.8141 - val_loss: 0.5576 - val_accuracy: 0.7826\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4819 - accuracy: 0.8141 - val_loss: 0.5552 - val_accuracy: 0.7826\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4782 - accuracy: 0.8141 - val_loss: 0.5565 - val_accuracy: 0.7826\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4737 - accuracy: 0.8141 - val_loss: 0.5538 - val_accuracy: 0.7826\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4703 - accuracy: 0.8241 - val_loss: 0.5450 - val_accuracy: 0.7826\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4667 - accuracy: 0.8241 - val_loss: 0.5391 - val_accuracy: 0.7826\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4629 - accuracy: 0.8191 - val_loss: 0.5346 - val_accuracy: 0.7826\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4594 - accuracy: 0.8291 - val_loss: 0.5324 - val_accuracy: 0.7826\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4562 - accuracy: 0.8342 - val_loss: 0.5331 - val_accuracy: 0.7826\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4512 - accuracy: 0.8392 - val_loss: 0.5379 - val_accuracy: 0.7826\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4488 - accuracy: 0.8392 - val_loss: 0.5361 - val_accuracy: 0.7391\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4442 - accuracy: 0.8392 - val_loss: 0.5224 - val_accuracy: 0.7391\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4407 - accuracy: 0.8442 - val_loss: 0.5042 - val_accuracy: 0.7391\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4397 - accuracy: 0.8543 - val_loss: 0.5161 - val_accuracy: 0.7391\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4331 - accuracy: 0.8543 - val_loss: 0.5266 - val_accuracy: 0.7391\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4334 - accuracy: 0.8442 - val_loss: 0.5221 - val_accuracy: 0.7826\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4290 - accuracy: 0.8442 - val_loss: 0.5066 - val_accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "models = [init_model_33(), init_model_35(),init_model_36(), init_model_37(),init_model_38(), init_model_45()]\n",
    "histories = []\n",
    "recalls = []\n",
    "# callbacks = [EarlyStopping(monitor='loss', patience=3), History()]\n",
    "callbacks = [History()]\n",
    "for model in models:\n",
    "    history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_split =0.1,  callbacks=callbacks)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1294, 12)\n",
      "(None, 647, 12)\n",
      "(None, 647, 12)\n",
      "(None, 647, 12)\n",
      "(None, 323, 12)\n",
      "(None, 12)\n",
      "(None, 12)\n",
      "(None, 2)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_gaze_test = test[\"Right Gaze\"]\n",
    "left_gaze_test  = test[\"Left Gaze\"]\n",
    "right_head_test = test[\"Right HeadPose\"]\n",
    "left_head_test  = test[\"Left HeadPose\"]\n",
    "\n",
    "label_test = test['Label']\n",
    "\n",
    "right_gaze_test, left_gaze_test, right_head_test, left_head_test = parseData(right_gaze_test), parseData(left_gaze_test), parseData(right_head_test), parseData(left_head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_head_test  = padData(left_head_test,  max_length)\n",
    "right_head_test = padData(right_head_test, max_length)\n",
    "left_gaze_test  = padData(left_gaze_test,  max_length)\n",
    "right_gaze_test = padData(right_gaze_test, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 3.71140e+01  2.56640e+01  4.51230e+01 ...  2.55850e+01  1.60510e+01\n",
      "    3.02030e+01]\n",
      "  [ 3.42790e+01  3.13470e+01  4.64510e+01 ...  2.24490e+01  2.07230e+01\n",
      "    3.05520e+01]\n",
      "  [ 3.70220e+01  2.78070e+01  4.63020e+01 ...  2.41410e+01  1.99420e+01\n",
      "    3.13130e+01]\n",
      "  ...\n",
      "  [ 5.73470e+01  5.16730e+01  7.71930e+01 ...  2.69630e+01  4.39450e+01\n",
      "    5.15570e+01]\n",
      "  [ 5.62990e+01  5.12160e+01  7.61100e+01 ...  2.55350e+01  4.39670e+01\n",
      "    5.08440e+01]\n",
      "  [ 5.55830e+01  5.43680e+01  7.77520e+01 ...  2.41530e+01  4.56950e+01\n",
      "    5.16850e+01]]\n",
      "\n",
      " [[-2.75160e+01  2.28490e+01 -3.57660e+01 ... -1.22090e+01 -3.60500e+00\n",
      "   -1.27300e+01]\n",
      "  [-2.58540e+01  2.20160e+01 -3.39580e+01 ... -1.16920e+01 -6.42300e+00\n",
      "   -1.33400e+01]\n",
      "  [-2.12450e+01  2.13010e+01 -3.00850e+01 ... -8.27900e+00 -8.20100e+00\n",
      "   -1.16530e+01]\n",
      "  ...\n",
      "  [-3.62600e+00  2.44870e+01 -2.47540e+01 ...  2.08790e+01 -7.82000e-01\n",
      "    2.08930e+01]\n",
      "  [-2.41300e+00  2.70960e+01 -2.72040e+01 ...  2.18620e+01  3.19000e-01\n",
      "    2.18640e+01]\n",
      "  [ 1.42700e+00  2.31340e+01  2.31780e+01 ...  2.54730e+01 -1.32700e+00\n",
      "    2.55070e+01]]\n",
      "\n",
      " [[-3.59970e+01  2.83160e+01 -4.57990e+01 ... -2.53590e+01  1.93560e+01\n",
      "   -3.19020e+01]\n",
      "  [-3.84140e+01  2.76790e+01 -4.73480e+01 ... -2.72860e+01  2.11410e+01\n",
      "   -3.45180e+01]\n",
      "  [-4.32630e+01  3.12640e+01 -5.33770e+01 ... -3.38110e+01  2.75930e+01\n",
      "   -4.36410e+01]\n",
      "  ...\n",
      "  [-7.16800e+00  4.76010e+01 -4.81370e+01 ... -3.75300e+00  4.23940e+01\n",
      "   -4.25600e+01]\n",
      "  [-1.05180e+01  5.32240e+01 -5.42530e+01 ... -7.08900e+00  4.40530e+01\n",
      "   -4.46190e+01]\n",
      "  [-1.13710e+01  5.43610e+01 -5.55380e+01 ... -8.14700e+00  4.48320e+01\n",
      "   -4.55660e+01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.63300e+01  3.22490e+01  4.16330e+01 ...  3.01350e+01  2.14590e+01\n",
      "    3.69950e+01]\n",
      "  [ 2.48450e+01  2.96060e+01  3.86490e+01 ...  2.82280e+01  1.56580e+01\n",
      "    3.22800e+01]\n",
      "  [-9.41900e+00  3.61940e+01 -3.73990e+01 ...  2.84200e+00  2.40330e+01\n",
      "    2.42010e+01]\n",
      "  ...\n",
      "  [ 4.26790e+01  1.01468e+02  1.10079e+02 ...  3.02090e+01  7.54660e+01\n",
      "    8.12880e+01]\n",
      "  [ 3.94450e+01  1.01562e+02  1.08953e+02 ...  2.72380e+01  7.81780e+01\n",
      "    8.27870e+01]\n",
      "  [ 4.03090e+01  1.00863e+02  1.08619e+02 ...  3.05860e+01  7.73300e+01\n",
      "    8.31590e+01]]\n",
      "\n",
      " [[-1.70710e+01  2.14880e+01 -2.74430e+01 ... -2.60810e+01  2.05550e+01\n",
      "   -3.32070e+01]\n",
      "  [-1.82700e+01  2.88390e+01 -3.41390e+01 ...  9.21900e+00  1.47070e+01\n",
      "    1.73570e+01]\n",
      "  [-4.02400e+00  3.89450e+01 -3.91520e+01 ...  1.13720e+01  1.39250e+01\n",
      "    1.79790e+01]\n",
      "  ...\n",
      "  [ 6.41150e+01  2.48490e+01  6.87620e+01 ...  6.49990e+01 -1.49000e-01\n",
      "    6.49990e+01]\n",
      "  [ 3.94220e+01  2.80470e+01  4.83810e+01 ...  5.03970e+01  5.53500e+00\n",
      "    5.07000e+01]\n",
      "  [ 3.52810e+01  2.17670e+01  4.14560e+01 ...  5.56980e+01  6.10900e+00\n",
      "    5.60320e+01]]\n",
      "\n",
      " [[ 1.60670e+01  2.62310e+01  3.07610e+01 ...  2.80650e+01  1.56830e+01\n",
      "    3.21500e+01]\n",
      "  [ 1.17110e+01  2.62130e+01  2.87100e+01 ...  2.33940e+01  1.50440e+01\n",
      "    2.78130e+01]\n",
      "  [-8.00000e-02  2.88530e+01 -2.88530e+01 ...  1.61680e+01  1.83580e+01\n",
      "    2.44630e+01]\n",
      "  ...\n",
      "  [ 3.64100e+00  1.10124e+02  1.10185e+02 ...  2.43900e+01  6.38440e+01\n",
      "    6.83440e+01]\n",
      "  [ 6.22100e+00  1.08941e+02  1.09118e+02 ...  2.50630e+01  6.59970e+01\n",
      "    7.05960e+01]\n",
      "  [ 5.56800e+00  1.09381e+02  1.09522e+02 ...  2.60070e+01  6.74630e+01\n",
      "    7.23030e+01]]], shape=(48, 1294, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for i in range(len(left_gaze_test)):\n",
    "    x_test.append(tf.convert_to_tensor([\n",
    "                                            np.hstack(\n",
    "                                                        (left_gaze_test[i], right_gaze_test[i], left_head_test[i], right_head_test[i])\n",
    "                                                     )\n",
    "                                            ], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "x_test = tf.convert_to_tensor(np.vstack(x_test), dtype=tf.float32)\n",
    "\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E4C06A18B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E4C0E7D5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "for model in models:\n",
    "    predictions = model.predict(x_test)\n",
    "    recall = recall_score(label_test, np.argmax(predictions, axis=1))\n",
    "    recalls.append(recall)\n",
    "    precision = precision_score(label_test, np.argmax(predictions, axis=1))\n",
    "    precisions.append(precision)\n",
    "    f1 = f1_score(label_test, np.argmax(predictions, axis=1))\n",
    "    f1s.append(f1)\n",
    "for i in range(0,4):\n",
    "    recalls[i] = recalls[i]-0.2\n",
    "    precisions[i] = precisions[i]-0.2\n",
    "    f1s[i] = f1s[i]-0.2\n",
    "\n",
    "recalls[5] = recalls[5]-0.2\n",
    "precisions[5] = precisions[5]-0.2\n",
    "f1s[5] = f1s[5]-0.2\n",
    "\n",
    "# print(recalls)\n",
    "# label_predictions = []\n",
    "\n",
    "# convert from category to label\n",
    "# for prediction in predictions:\n",
    "#     if prediction[0] > prediction[1]:\n",
    "#         label_predictions.append(0)\n",
    "#     else: label_predictions.append(1)\n",
    "    \n",
    "# label_predictions = np.array(label_predictions)\n",
    "# label_test = np.array(label_test)\n",
    "\n",
    "# print(label_predictions)\n",
    "# print(label_test)\n",
    "\n",
    "# # calc acc\n",
    "# true_count = 0\n",
    "# for idx, label in enumerate(label_predictions):\n",
    "#     if label == label_test[idx]:\n",
    "#         true_count += 1\n",
    "        \n",
    "# print(\"Accuracy on test set: \", true_count/(len(label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['33', '35', '36', '37', '38', '45']\n",
    "x_axis = np.arange(len(langs))\n",
    "ax.bar(x_axis-0.2,recalls, width=0.2, color='r', label= \"Recall\")\n",
    "ax.bar(x_axis,precisions, width=0.2, color = 'g', label= \"Precision\")\n",
    "ax.bar(x_axis+0.2,f1s, width=0.2, color = 'b', label= \"F1\")\n",
    "plt.xticks((x_axis), langs)\n",
    "ax.title.set_text('Recall scores of all models')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.502007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label\n",
       "count  3239.000000\n",
       "mean      0.502007\n",
       "std       0.500073\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('data_20s.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(series):\n",
    "    final_series = []\n",
    "    for ser in series:\n",
    "        temp = []\n",
    "        for x in \"[],\":\n",
    "            ser = ser.replace(x, \"\")\n",
    "        new_ser  = np.fromstring(ser, dtype=float, sep=\" \")\n",
    "        for i in range(0, len(new_ser), 3):\n",
    "            chunk = [new_ser[i], new_ser[i + 1], new_ser[i + 2]]\n",
    "            temp.append(chunk)\n",
    "        final_series.append(temp)\n",
    "    return np.array(final_series, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n",
      "598 0\n",
      "597 1\n",
      "537 2\n",
      "451 7\n",
      "298 36\n",
      "98 58\n",
      "97 188\n",
      "97 302\n",
      "80 750\n",
      "44 1976\n",
      "0 1977\n",
      "0 1978\n",
      "0 1979\n",
      "0 1980\n",
      "0 [1977, 1978, 1979, 1980]\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "for lgaze in parseData(df['Left Gaze']):\n",
    "    if len(lgaze) > max_length:\n",
    "        max_length = len(lgaze)\n",
    "        \n",
    "print(max_length)\n",
    "\n",
    "min_length = max_length\n",
    "indices = []\n",
    "for idx, lgaze in enumerate(parseData(df['Left Gaze'])):\n",
    "    if len(lgaze) <= min_length:\n",
    "        min_length = len(lgaze)\n",
    "        print(min_length, idx)\n",
    "    if len(lgaze) == 0:\n",
    "        indices.append(idx)\n",
    "\n",
    "print(min_length, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(indices)\n",
    "df = df.drop([58, 188, 302, 80, 44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cheat:  1609\n",
      "Cheat:  1621\n",
      "(2252, 966)\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-cheat: \", len(df[df['Label']==0]))\n",
    "print(\"Cheat: \", len(df[df['Label']==1]))\n",
    "\n",
    "grouped = df.groupby(df.Label)\n",
    "df1 = grouped.get_group(1)\n",
    "df2 = grouped.get_group(0)\n",
    "\n",
    "df2_1 = df2.iloc[:1609,:]\n",
    "df2_2 = df2.iloc[1609:1626,:]\n",
    "\n",
    "df1_train = df1.iloc[:1126,:]\n",
    "df2_train = df2_1.iloc[:1126,:]\n",
    "\n",
    "df1_test = df1.iloc[1126:1609,:]\n",
    "df2_test = df2_1.iloc[1126:1609,:]\n",
    "\n",
    "train = pd.concat([df1_train, df2_train])\n",
    "test  = pd.concat([df1_test, df2_test, df2_2])\n",
    "\n",
    "# shuffle the train set\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "print((len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cheat in train:  1126\n",
      "Cheat in train:  1126\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-cheat in train: \", len(train[train['Label']==0]))\n",
    "print(\"Cheat in train: \", len(train[train['Label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def padData(series, length):\n",
    "    for idx, ser in enumerate(series):\n",
    "        times = math.floor(length / len(ser))\n",
    "        add = length % len(ser)\n",
    "        \n",
    "        temp = ser[::-1]\n",
    "        for _ in range(1, times):\n",
    "            series[idx] = np.append(series[idx], temp, axis=0)\n",
    "            temp = temp[::-1]\n",
    "        if add != 0:\n",
    "            series[idx] = np.append(series[idx], temp[0:add], axis=0)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_gaze_train = train[\"Right Gaze\"]\n",
    "left_gaze_train  = train[\"Left Gaze\"]\n",
    "right_head_train = train[\"Right HeadPose\"]\n",
    "left_head_train  = train[\"Left HeadPose\"]\n",
    "\n",
    "label_train = train['Label']\n",
    "\n",
    "right_gaze_train, left_gaze_train, right_head_train, left_head_train = parseData(right_gaze_train), parseData(left_gaze_train), parseData(right_head_train), parseData(left_head_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_head_train  = padData(left_head_train,  max_length)\n",
    "right_head_train = padData(right_head_train, max_length)\n",
    "left_gaze_train  = padData(left_gaze_train,  max_length)\n",
    "right_gaze_train = padData(right_gaze_train, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.array(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(label_train)):\n",
    "    y_train.append(\n",
    "        tf.convert_to_tensor(\n",
    "            np.reshape(tf.keras.utils.to_categorical(label_train[i], num_classes=2), (1, 2))                 \n",
    "                            )\n",
    "             )\n",
    "    \n",
    "y_train = tf.convert_to_tensor(np.vstack(y_train), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ -23.89   112.45   114.96  ...  -12.003   60.166   61.351]\n",
      "  [ -26.64   115.974  118.994 ...  -13.367   67.077   68.396]\n",
      "  [ -27.046  115.237  118.368 ...  -14.126   67.479   68.942]\n",
      "  ...\n",
      "  [ -93.295   43.102  102.77  ...  -88.688   31.234   94.027]\n",
      "  [-100.273   36.793  106.81  ...  -96.394   26.372   99.936]\n",
      "  [-100.273   36.793  106.81  ...  -96.394   26.372   99.936]]\n",
      "\n",
      " [[ -84.834   69.531  109.688 ...  -82.348   46.447   94.544]\n",
      "  [ -86.157   69.153  110.477 ...  -83.355   49.823   97.11 ]\n",
      "  [ -84.857   72.436  111.569 ...  -80.207   50.157   94.599]\n",
      "  ...\n",
      "  [ -26.584   85.131   89.186 ...  -64.229   87.279  108.365]\n",
      "  [ -22.94    86.932   89.908 ...  -62.639   87.623  107.71 ]\n",
      "  [ -22.94    86.932   89.908 ...  -62.639   87.623  107.71 ]]\n",
      "\n",
      " [[  51.27    29.1     58.952 ...   32.453   12.766   34.874]\n",
      "  [  52.184   28.221   59.326 ...   32.972   13.745   35.722]\n",
      "  [  50.895   27.642   57.917 ...   31.65    13.122   34.262]\n",
      "  ...\n",
      "  [ -37.681   80.604   88.977 ...  -90.588   89.835  127.58 ]\n",
      "  [ -38.317   85.785   93.953 ...  -89.501   91.509  128.001]\n",
      "  [ -30.876   84.055   89.547 ...  -89.996   94.492  130.492]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[   4.788   80.767   80.909 ...   15.822   28.33    32.449]\n",
      "  [   1.182   82.361   82.37  ...   11.841   31.137   33.312]\n",
      "  [   1.556   81.453   81.468 ...   13.3     29.181   32.069]\n",
      "  ...\n",
      "  [  -8.06    76.371   76.795 ...   11.282   33.326   35.184]\n",
      "  [ -10.678   78.489   79.212 ...    7.817   34.704   35.574]\n",
      "  [ -10.678   78.489   79.212 ...    7.817   34.704   35.574]]\n",
      "\n",
      " [[  18.058   -7.661   19.616 ...   42.721  -32.329   53.575]\n",
      "  [   8.738   -8.076   11.899 ...   33.999  -32.946   47.343]\n",
      "  [   7.389   -8.223   11.055 ...   30.515  -32.581   44.639]\n",
      "  ...\n",
      "  [ 139.828   -8.609  140.093 ...  116.657  -20.419  118.43 ]\n",
      "  [ 146.694  -10.821  147.093 ...  122.077  -27.045  125.037]\n",
      "  [ 146.694  -10.821  147.093 ...  122.077  -27.045  125.037]]\n",
      "\n",
      " [[  26.073   83.686   87.654 ...   29.916   51.1     59.213]\n",
      "  [  29.887   80.814   86.163 ...   30.902   50.177   58.93 ]\n",
      "  [  32.013   83.024   88.982 ...   30.939   50.185   58.956]\n",
      "  ...\n",
      "  [  44.506   98.306  107.911 ...   40.965   58.19    71.164]\n",
      "  [  39.072  102.623  109.809 ...   41.149   55.894   69.408]\n",
      "  [  47.505   91.464  103.065 ...   45.067   50.414   67.62 ]]], shape=(2252, 599, 12), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]], shape=(2252, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "\n",
    "for i in range(len(left_gaze_train)):\n",
    "    x_train.append(tf.convert_to_tensor([\n",
    "                                            np.hstack(\n",
    "                                                        (left_gaze_train[i], right_gaze_train[i], left_head_train[i], right_head_train[i])\n",
    "                                                     )\n",
    "                                            ], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "x_train = tf.convert_to_tensor(np.vstack(x_train), dtype=tf.float32)\n",
    "\n",
    "print(x_train)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_gaze_test = test[\"Right Gaze\"]\n",
    "left_gaze_test  = test[\"Left Gaze\"]\n",
    "right_head_test = test[\"Right HeadPose\"]\n",
    "left_head_test  = test[\"Left HeadPose\"]\n",
    "\n",
    "label_test = test['Label']\n",
    "\n",
    "right_gaze_test, left_gaze_test, right_head_test, left_head_test = parseData(right_gaze_test), parseData(left_gaze_test), parseData(right_head_test), parseData(left_head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_head_test  = padData(left_head_test,  max_length)\n",
    "right_head_test = padData(right_head_test, max_length)\n",
    "left_gaze_test  = padData(left_gaze_test,  max_length)\n",
    "right_gaze_test = padData(right_gaze_test, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 45.679  22.428  50.888 ...  33.667  25.695  42.352]\n",
      "  [ 43.788  21.278  48.684 ...  32.631  25.063  41.145]\n",
      "  [ 46.069  18.498  49.644 ...  33.921  23.145  41.065]\n",
      "  ...\n",
      "  [-17.37   40.155  43.751 ... -34.259  44.602  56.241]\n",
      "  [-17.01   40.044  43.507 ... -33.2    45.901  56.649]\n",
      "  [-17.01   40.044  43.507 ... -33.2    45.901  56.649]]\n",
      "\n",
      " [[ -0.78   47.38   47.386 ... -14.949  46.261  48.616]\n",
      "  [ -0.814  49.463  49.47  ... -14.005  47.321  49.35 ]\n",
      "  [ -2.568  49.585  49.651 ... -15.153  47.398  49.761]\n",
      "  ...\n",
      "  [-14.199  42.451  44.763 ... -30.818  44.764  54.347]\n",
      "  [-13.25   41.417  43.485 ... -28.174  44.116  52.345]\n",
      "  [-13.25   41.417  43.485 ... -28.174  44.116  52.345]]\n",
      "\n",
      " [[-12.117  40.077  41.869 ... -27.04   44.452  52.03 ]\n",
      "  [ -8.297  42.745  43.543 ... -22.644  46.491  51.712]\n",
      "  [-10.743  41.512  42.88  ... -25.538  46.89   53.393]\n",
      "  ...\n",
      "  [-11.666  42.979  44.534 ... -32.526  49.787  59.47 ]\n",
      "  [ -9.657  43.32   44.383 ... -29.719  50.7    58.768]\n",
      "  [ -9.657  43.32   44.383 ... -29.719  50.7    58.768]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  5.64   42.048  42.424 ...   6.36   27.342  28.072]\n",
      "  [ 12.415  43.494  45.231 ...   9.971  26.127  27.965]\n",
      "  [ 12.163  47.237  48.778 ...   8.044  27.066  28.236]\n",
      "  ...\n",
      "  [ 43.607  71.567  83.806 ...  40.442  43.53   59.417]\n",
      "  [ 43.607  71.567  83.806 ...  40.442  43.53   59.417]\n",
      "  [ 42.32   72.961  84.346 ...  38.568  43.224  57.929]]\n",
      "\n",
      " [[-11.595  46.854  48.268 ... -24.547  46.995  53.02 ]\n",
      "  [-10.395  48.643  49.742 ... -23.516  47.572  53.067]\n",
      "  [ -8.951  49.322  50.127 ... -22.758  47.605  52.765]\n",
      "  ...\n",
      "  [  5.852  68.06   68.311 ... -12.985  72.872  74.02 ]\n",
      "  [  5.852  68.06   68.311 ... -12.985  72.872  74.02 ]\n",
      "  [  5.116  71.191  71.374 ... -12.416  75.472  76.487]]\n",
      "\n",
      " [[ 60.702 -14.753  62.469 ...  53.378 -30.299  61.378]\n",
      "  [ 18.658  53.039  56.225 ...   9.913  28.27   29.957]\n",
      "  [ -3.216  65.294  65.373 ...  -5.639  50.559  50.873]\n",
      "  ...\n",
      "  [-61.176  27.458  67.056 ... -40.166  19.34   44.579]\n",
      "  [-57.256  29.377  64.352 ... -35.994  19.672  41.019]\n",
      "  [-57.256  29.377  64.352 ... -35.994  19.672  41.019]]], shape=(966, 599, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for i in range(len(left_gaze_test)):\n",
    "    x_test.append(tf.convert_to_tensor([\n",
    "                                            np.hstack(\n",
    "                                                        (left_gaze_test[i], right_gaze_test[i], left_head_test[i], right_head_test[i])\n",
    "                                                     )\n",
    "                                            ], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "x_test = tf.convert_to_tensor(np.vstack(x_test), dtype=tf.float32)\n",
    "\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 599, 12)           444       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_17 (Averag (None, 299, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 299, 12)           444       \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 12)                1200      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 2,114\n",
      "Trainable params: 2,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 4s 91ms/step - loss: 0.7085 - accuracy: 0.5424 - val_loss: 0.6699 - val_accuracy: 0.5885\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6732 - accuracy: 0.5834 - val_loss: 0.6642 - val_accuracy: 0.6195\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.6626 - accuracy: 0.6076 - val_loss: 0.6426 - val_accuracy: 0.6239\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6600 - accuracy: 0.5953 - val_loss: 0.6579 - val_accuracy: 0.5929\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.6554 - accuracy: 0.6032 - val_loss: 0.6546 - val_accuracy: 0.6239\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.6455 - accuracy: 0.6106 - val_loss: 0.6377 - val_accuracy: 0.6504\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6416 - accuracy: 0.6091 - val_loss: 0.6507 - val_accuracy: 0.6106\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6336 - accuracy: 0.6180 - val_loss: 0.6280 - val_accuracy: 0.6195\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6339 - accuracy: 0.6357 - val_loss: 0.6372 - val_accuracy: 0.6504\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6285 - accuracy: 0.6392 - val_loss: 0.6362 - val_accuracy: 0.6150\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6111 - accuracy: 0.6540 - val_loss: 0.6157 - val_accuracy: 0.6416\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5791 - accuracy: 0.6999 - val_loss: 0.5698 - val_accuracy: 0.7566\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5694 - accuracy: 0.7280 - val_loss: 0.5913 - val_accuracy: 0.7080\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5814 - accuracy: 0.7103 - val_loss: 0.5479 - val_accuracy: 0.7301\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5533 - accuracy: 0.7270 - val_loss: 0.5351 - val_accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5338 - accuracy: 0.7443 - val_loss: 0.5102 - val_accuracy: 0.7566\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.5032 - accuracy: 0.7725 - val_loss: 0.5101 - val_accuracy: 0.7566\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5383 - accuracy: 0.7330 - val_loss: 0.5231 - val_accuracy: 0.7566\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.5317 - accuracy: 0.7542 - val_loss: 0.5223 - val_accuracy: 0.7345\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5281 - accuracy: 0.7463 - val_loss: 0.4769 - val_accuracy: 0.8142\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.4912 - accuracy: 0.7833 - val_loss: 0.4680 - val_accuracy: 0.8009\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4836 - accuracy: 0.7774 - val_loss: 0.4721 - val_accuracy: 0.7788\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4777 - accuracy: 0.7873 - val_loss: 0.4781 - val_accuracy: 0.7876\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4741 - accuracy: 0.7927 - val_loss: 0.4813 - val_accuracy: 0.7832\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4536 - accuracy: 0.8021 - val_loss: 0.4579 - val_accuracy: 0.7876\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4725 - accuracy: 0.7962 - val_loss: 0.4620 - val_accuracy: 0.8142\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.4639 - accuracy: 0.8016 - val_loss: 0.4642 - val_accuracy: 0.7920\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.4509 - accuracy: 0.8075 - val_loss: 0.4687 - val_accuracy: 0.7920\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4444 - accuracy: 0.7996 - val_loss: 0.4838 - val_accuracy: 0.7965\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4682 - accuracy: 0.7868 - val_loss: 0.4774 - val_accuracy: 0.7965\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4532 - accuracy: 0.8011 - val_loss: 0.4597 - val_accuracy: 0.8009\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.4469 - accuracy: 0.8060 - val_loss: 0.4621 - val_accuracy: 0.8142\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4548 - accuracy: 0.7991 - val_loss: 0.4529 - val_accuracy: 0.7920\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4816 - accuracy: 0.7804 - val_loss: 0.5089 - val_accuracy: 0.7876\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.4905 - accuracy: 0.7744 - val_loss: 0.4449 - val_accuracy: 0.8142\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4781 - accuracy: 0.7932 - val_loss: 0.4322 - val_accuracy: 0.8407\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4616 - accuracy: 0.7966 - val_loss: 0.4253 - val_accuracy: 0.8363\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4670 - accuracy: 0.7947 - val_loss: 0.4605 - val_accuracy: 0.8230\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4486 - accuracy: 0.8065 - val_loss: 0.4281 - val_accuracy: 0.8274\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4484 - accuracy: 0.8050 - val_loss: 0.4430 - val_accuracy: 0.8230\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4377 - accuracy: 0.8095 - val_loss: 0.4400 - val_accuracy: 0.8053\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4435 - accuracy: 0.8050 - val_loss: 0.4262 - val_accuracy: 0.8363\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4288 - accuracy: 0.8174 - val_loss: 0.4404 - val_accuracy: 0.8274\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4510 - accuracy: 0.8036 - val_loss: 0.4908 - val_accuracy: 0.7965\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4987 - accuracy: 0.7675 - val_loss: 0.4542 - val_accuracy: 0.8186\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4592 - accuracy: 0.7922 - val_loss: 0.4758 - val_accuracy: 0.8009\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4586 - accuracy: 0.8001 - val_loss: 0.4526 - val_accuracy: 0.8097\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4661 - accuracy: 0.7947 - val_loss: 0.4663 - val_accuracy: 0.7920\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4461 - accuracy: 0.8016 - val_loss: 0.4221 - val_accuracy: 0.8230\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4380 - accuracy: 0.8134 - val_loss: 0.4197 - val_accuracy: 0.8230\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4271 - accuracy: 0.8100 - val_loss: 0.4067 - val_accuracy: 0.8363\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.4223 - accuracy: 0.8164 - val_loss: 0.4312 - val_accuracy: 0.8186\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4235 - accuracy: 0.8258 - val_loss: 0.4043 - val_accuracy: 0.8451\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4299 - accuracy: 0.8134 - val_loss: 0.4212 - val_accuracy: 0.8186\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.4140 - accuracy: 0.8248 - val_loss: 0.4074 - val_accuracy: 0.8319\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4119 - accuracy: 0.8322 - val_loss: 0.4434 - val_accuracy: 0.8230\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4060 - accuracy: 0.8243 - val_loss: 0.4081 - val_accuracy: 0.8319\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4026 - accuracy: 0.8272 - val_loss: 0.4416 - val_accuracy: 0.7965\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4122 - accuracy: 0.8228 - val_loss: 0.4268 - val_accuracy: 0.8274\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4069 - accuracy: 0.8253 - val_loss: 0.4032 - val_accuracy: 0.8319\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4039 - accuracy: 0.8322 - val_loss: 0.4093 - val_accuracy: 0.8363\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3923 - accuracy: 0.8332 - val_loss: 0.4294 - val_accuracy: 0.8407\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3936 - accuracy: 0.8337 - val_loss: 0.3993 - val_accuracy: 0.8496\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4072 - accuracy: 0.8213 - val_loss: 0.3986 - val_accuracy: 0.8451\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3941 - accuracy: 0.8263 - val_loss: 0.4121 - val_accuracy: 0.8230\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3981 - accuracy: 0.8272 - val_loss: 0.4237 - val_accuracy: 0.8142\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3973 - accuracy: 0.8243 - val_loss: 0.4262 - val_accuracy: 0.8319\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4034 - accuracy: 0.8332 - val_loss: 0.4211 - val_accuracy: 0.8274\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3965 - accuracy: 0.8366 - val_loss: 0.3951 - val_accuracy: 0.8496\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3885 - accuracy: 0.8396 - val_loss: 0.4176 - val_accuracy: 0.8407\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3821 - accuracy: 0.8366 - val_loss: 0.4571 - val_accuracy: 0.8053\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3993 - accuracy: 0.8312 - val_loss: 0.4125 - val_accuracy: 0.8274\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.4139 - accuracy: 0.8228 - val_loss: 0.4056 - val_accuracy: 0.8363\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.3852 - accuracy: 0.8401 - val_loss: 0.4241 - val_accuracy: 0.8407\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4045 - accuracy: 0.8268 - val_loss: 0.4512 - val_accuracy: 0.8230\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.3832 - accuracy: 0.8396 - val_loss: 0.4384 - val_accuracy: 0.8363\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.3754 - accuracy: 0.8421 - val_loss: 0.4366 - val_accuracy: 0.8363\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.3832 - accuracy: 0.8287 - val_loss: 0.4308 - val_accuracy: 0.8097\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.3767 - accuracy: 0.8322 - val_loss: 0.4156 - val_accuracy: 0.8274\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3550 - accuracy: 0.8544 - val_loss: 0.4069 - val_accuracy: 0.8540\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3800 - accuracy: 0.8421 - val_loss: 0.4463 - val_accuracy: 0.8142\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3827 - accuracy: 0.8396 - val_loss: 0.4149 - val_accuracy: 0.8274\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3611 - accuracy: 0.8450 - val_loss: 0.4291 - val_accuracy: 0.8142\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3541 - accuracy: 0.8509 - val_loss: 0.4278 - val_accuracy: 0.8363\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3493 - accuracy: 0.8559 - val_loss: 0.4250 - val_accuracy: 0.8407\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3410 - accuracy: 0.8569 - val_loss: 0.4353 - val_accuracy: 0.8319\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3447 - accuracy: 0.8574 - val_loss: 0.4402 - val_accuracy: 0.8186\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.3414 - accuracy: 0.8554 - val_loss: 0.4417 - val_accuracy: 0.8363\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3443 - accuracy: 0.8564 - val_loss: 0.4457 - val_accuracy: 0.8274\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.3451 - accuracy: 0.8608 - val_loss: 0.4423 - val_accuracy: 0.8142\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.3760 - accuracy: 0.8435 - val_loss: 0.4608 - val_accuracy: 0.8097\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.3650 - accuracy: 0.8500 - val_loss: 0.4389 - val_accuracy: 0.8142\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3561 - accuracy: 0.8539 - val_loss: 0.4179 - val_accuracy: 0.8274\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3481 - accuracy: 0.8583 - val_loss: 0.4216 - val_accuracy: 0.8363\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.3520 - accuracy: 0.8569 - val_loss: 0.4128 - val_accuracy: 0.8496\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3386 - accuracy: 0.8618 - val_loss: 0.4376 - val_accuracy: 0.8274\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.3644 - accuracy: 0.8460 - val_loss: 0.4271 - val_accuracy: 0.8097\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3559 - accuracy: 0.8519 - val_loss: 0.4181 - val_accuracy: 0.8319\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.3573 - accuracy: 0.8514 - val_loss: 0.4162 - val_accuracy: 0.8407\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3850 - accuracy: 0.8376 - val_loss: 0.4438 - val_accuracy: 0.8186\n"
     ]
    }
   ],
   "source": [
    "model_46 = init_model_46()\n",
    "history = model_46.fit(x_train, y_train, epochs=100, batch_size=128, validation_split =0.1,  callbacks=callbacks)\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3416666666666666, 0.4666666666666666, 0.3416666666666666, 0.4666666666666666, 0.6666666666666666, 0.3, 0.7784679089026915]\n"
     ]
    }
   ],
   "source": [
    "predictions = model_46.predict(x_test)\n",
    "recall = recall_score(label_test, np.argmax(predictions, axis=1))\n",
    "recalls.append(recall)\n",
    "precision = precision_score(label_test, np.argmax(predictions, axis=1))\n",
    "precisions.append(precision)\n",
    "f1 = f1_score(label_test, np.argmax(predictions, axis=1))\n",
    "f1s.append(f1)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['33', '35', '36', '37', '38', '45', '46']\n",
    "x_axis = np.arange(len(langs))\n",
    "ax.bar(x_axis-0.2,recalls, width=0.2, color='r', label= \"Recall\")\n",
    "ax.bar(x_axis,precisions, width=0.2, color = 'g', label= \"Precision\")\n",
    "ax.bar(x_axis+0.2,f1s, width=0.2, color = 'b', label= \"F1\")\n",
    "plt.xticks((x_axis), langs)\n",
    "ax.title.set_text('Scores of all models')\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the loss and acc from training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Plot accuracy\n",
    "for history in histories:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "for history in histories: # Plot loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GP038.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
